# ADR-0010: Async Search Queue Architecture

## Date
2025-12-10

## Context

Webæ¤œç´¢ãƒ»ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¯æ™‚é–“ãŒã‹ã‹ã‚‹æ“ä½œã§ã‚ã‚‹ï¼š

| æ“ä½œ | æ‰€è¦æ™‚é–“ |
|------|----------|
| æ¤œç´¢APIå‘¼ã³å‡ºã— | 1-3ç§’ |
| ãƒšãƒ¼ã‚¸å–å¾— | 2-10ç§’ |
| JavaScriptå®Ÿè¡Œå¾…ã¡ | 3-15ç§’ |
| LLMæŠ½å‡º | 1-5ç§’ |

åŒæœŸçš„ã«å‡¦ç†ã™ã‚‹ã¨ï¼š
- 10ãƒšãƒ¼ã‚¸ Ã— 10ç§’ = 100ç§’å¾…æ©Ÿ
- MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ãŒæ‚ªåŒ–

## Decision

**æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚­ãƒ¥ãƒ¼ã«æŠ•å…¥ã—ã€éåŒæœŸã§å‡¦ç†ã™ã‚‹ã€‚ãƒãƒ¼ãƒªãƒ³ã‚°ã§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèªã™ã‚‹ã€‚**

### Scheduling Policy (Ordering / Concurrency)

- **Ordering**: The worker processes jobs by **priority ASC**, then **created_at ASC** (FIFO within the same priority).
- **No cross-task fairness**: There is no round-robin or fairness logic across tasks beyond the ordering rule above.
- **No per-task sequential guarantee**: A task may have multiple searches running in parallel.
- **Priority vocabulary**: If priority is exposed, use `high | medium | low` (align with existing Lyra terminology).

### Worker Lifecycle

- The queue worker is started when the MCP server starts (`run_server()` startup).
- The queue worker is stopped (cancelled) when the MCP server shuts down (`run_server()` shutdown).
- Start **2 worker tasks** for parallel execution.

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ                          Lyra
     â”‚                                    â”‚
     â”‚  queue_searches([q1, q2, q3])      â”‚
     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚                                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  {task_id: "xxx", queued: 3}       â”‚ â”‚  Search Queue   â”‚
     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”‚  [q1, q2, q3]   â”‚
     â”‚                                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                                    â”‚          â”‚
     â”‚  (MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ä»–ã®ä½œæ¥­)         â”‚          â–¼ éåŒæœŸå‡¦ç†
     â”‚                                    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  get_status(task_id, wait=30)      â”‚   â”‚   Worker     â”‚
     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   â”‚  - Crawl     â”‚
     â”‚                                    â”‚   â”‚  - Extract   â”‚
     â”‚  {progress: "2/3", results: [...]} â”‚   â”‚  - Store     â”‚
     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                                    â”‚
```

### MCPãƒ„ãƒ¼ãƒ«è¨­è¨ˆ

#### queue_searches

```python
@server.tool()
async def queue_searches(
    task_id: str,
    queries: List[str],
    max_results_per_query: int = 10
) -> QueueResult:
    """
    æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆå³åº§ã«è¿”å´ï¼‰

    Returns:
        task_id: ã‚¿ã‚¹ã‚¯è­˜åˆ¥å­
        queued: ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã•ã‚ŒãŸæ•°
        estimated_time: æ¨å®šå®Œäº†æ™‚é–“
    """
    for query in queries:
        await search_queue.enqueue(task_id, query, max_results_per_query)

    return QueueResult(
        task_id=task_id,
        queued=len(queries),
        estimated_time=estimate_completion_time(queries)
    )
```

#### get_statusï¼ˆsleepå¯¾å¿œï¼‰
#### get_statusï¼ˆwait / long pollingï¼‰

```python
@server.tool()
async def get_status(
    task_id: str,
    wait: int = 0  # ç§’æ•°ã€0ãªã‚‰å³åº§ã«è¿”å´
) -> StatusResult:
    """
    ã‚¿ã‚¹ã‚¯ã®é€²æ—ã‚’å–å¾—

    wait > 0 ã®å ´åˆã€å®Œäº†ã¾ãŸã¯å¤‰åŒ–ãŒã‚ã‚‹ã¾ã§æœ€å¤§waitç§’å¾…æ©Ÿ
    """
    if wait > 0:
        # Long polling: å¤‰åŒ–ãŒã‚ã‚‹ã¾ã§å¾…æ©Ÿ
        result = await wait_for_progress(task_id, timeout=wait)
    else:
        result = await get_current_status(task_id)

    return StatusResult(
        task_id=task_id,
        status=result.status,  # "running", "completed", "failed"
        progress=f"{result.completed}/{result.total}",
        results=result.available_results,
        errors=result.errors
    )
```

### Long Polling ã®åˆ©ç‚¹

```
å¾“æ¥ï¼ˆçŸ­ã„ãƒãƒ¼ãƒªãƒ³ã‚°ï¼‰:
  Client: get_status â†’ Server: {progress: "0/3"}
  (1ç§’å¾Œ)
  Client: get_status â†’ Server: {progress: "0/3"}
  (1ç§’å¾Œ)
  Client: get_status â†’ Server: {progress: "1/3"}
  ...
  â†’ ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤šã„ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒæ‚ªã„

Long Polling:
  Client: get_status(wait=30) â†’ (ã‚µãƒ¼ãƒãƒ¼ã§å¾…æ©Ÿ)
  (é€²æ—ãŒã‚ã£ãŸæ™‚ç‚¹ã§)
  Server: {progress: "1/3"}
  â†’ ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰Šæ¸›ã€å³åº§ã«é€šçŸ¥
```

### ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè£…

```python
class SearchWorker:
    async def process_queue(self):
        while True:
            # IMPORTANT: dequeue must be atomic when multiple workers run.
            # Use "claim" semantics (queued -> running) in a single DB operation
            # (e.g., UPDATE ... RETURNING) or a transaction (BEGIN IMMEDIATE)
            # to avoid two workers picking the same queued item.
            job = await self.queue.dequeue()
            if job is None:
                await asyncio.sleep(0.1)
                continue

            try:
                # æ¤œç´¢å®Ÿè¡Œ
                results = await self.search_engine.search(job.query)

                # å„çµæœã‚’ã‚¯ãƒ­ãƒ¼ãƒ«
                for url in results[:job.max_results]:
                    page = await self.crawler.fetch(url)
                    await self.storage.save_page(page)

                    # é€²æ—ã‚’æ›´æ–°ï¼ˆLong pollingã«é€šçŸ¥ï¼‰
                    await self.notify_progress(job.task_id)

            except Exception as e:
                await self.record_error(job.task_id, e)
```

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
@dataclass
class StatusResult:
    task_id: str
    status: str
    progress: str
    results: List[PageSummary]
    errors: List[ErrorInfo]  # éƒ¨åˆ†çš„ãªã‚¨ãƒ©ãƒ¼ã‚‚å ±å‘Š

# ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦ã‚‚å‡¦ç†ç¶™ç¶š
{
    "status": "running",
    "progress": "8/10",
    "results": [...],  # æˆåŠŸã—ãŸ8ä»¶
    "errors": [
        {"url": "https://...", "reason": "timeout"},
        {"url": "https://...", "reason": "403 Forbidden"}
    ]
}
```

## Consequences

### Positive
- **éãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°**: MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒå¾…æ©Ÿä¸è¦
- **ä¸¦åˆ—å‡¦ç†**: è¤‡æ•°ã‚¯ã‚¨ãƒªã‚’åŒæ™‚å‡¦ç†å¯èƒ½
- **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿**: é•·æ™‚é–“å‡¦ç†ã§ã‚‚MCPã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãªã„
- **éƒ¨åˆ†çµæœ**: å®Œäº†å‰ã§ã‚‚é€”ä¸­çµæœã‚’å–å¾—å¯èƒ½

### Negative
- **è¤‡é›‘æ€§**: ã‚­ãƒ¥ãƒ¼ç®¡ç†ã€ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†ãŒå¿…è¦
- **çŠ¶æ…‹ç®¡ç†**: ã‚¿ã‚¹ã‚¯çŠ¶æ…‹ã®æ°¸ç¶šåŒ–ãŒå¿…è¦
- **ãƒ‡ãƒãƒƒã‚°å›°é›£**: éåŒæœŸå‡¦ç†ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãŒè¤‡é›‘

## Alternatives Considered

| Alternative | Pros | Cons | åˆ¤å®š |
|-------------|------|------|------|
| åŒæœŸå‡¦ç† | ã‚·ãƒ³ãƒ—ãƒ« | ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå•é¡Œ | å´ä¸‹ |
| WebSocket | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  | å®Ÿè£…è¤‡é›‘ã€MCPéå¯¾å¿œ | å´ä¸‹ |
| çŸ­ã„ãƒãƒ¼ãƒªãƒ³ã‚° | ã‚·ãƒ³ãƒ—ãƒ« | ãƒªã‚¯ã‚¨ã‚¹ãƒˆéå¤š | å´ä¸‹ |
| Server-Sent Events | è»½é‡ | MCPéå¯¾å¿œ | å´ä¸‹ |

## Implementation Status

**Status**: Phase 1-3 âœ… å®Œäº† / Phase 4 ğŸ”œ è¨ˆç”»ä¸­

è©³ç´°ã¯ `docs/Q_ASYNC_ARCHITECTURE.md` ã‚’å‚ç…§ã€‚

### ãƒ•ã‚§ãƒ¼ã‚ºä¸€è¦§

| Phase | å†…å®¹ | çŠ¶æ…‹ |
|-------|------|------|
| Phase 1 | `queue_searches` ãƒ„ãƒ¼ãƒ«è¿½åŠ ã€`get_status` ã« `wait` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ  | âœ… å®Œäº† (2025-12-24) |
| Phase 2 | `search`, `notify_user`, `wait_for_user` ãƒ„ãƒ¼ãƒ«å‰Šé™¤ | âœ… å®Œäº† (2025-12-24) |
| Phase 3 | ä¸€æ¬¡æ¤œè¨¼ã€`stop_task` ã® `mode` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆgraceful/immediateï¼‰è¿½åŠ  | âœ… å®Œäº† (2025-12-24) |
| Phase 4 | ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆåˆ¶å¾¡ï¼ˆå­¦è¡“APIã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼‰ | ğŸ”œ è¨ˆç”»ä¸­ ([ADR-0013](0013-worker-resource-contention.md)) |

### Phase 1-3 å®Ÿè£…ã‚µãƒãƒªãƒ¼

| å¤‰æ›´ | çŠ¶æ…‹ |
|------|------|
| `search`, `notify_user`, `wait_for_user` å‰Šé™¤ | âœ… å®Œäº† |
| `queue_searches` è¿½åŠ  | âœ… å®Œäº† |
| `get_status` ã« `wait` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ  | âœ… å®Œäº† |
| `stop_task` ã« `mode` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¿½åŠ  | âœ… å®Œäº† |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹/å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ | âœ… å®Œäº† |
| E2Eæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ | âœ… å®Œäº† |
| çµæœ | 13ãƒ„ãƒ¼ãƒ« â†’ 10ãƒ„ãƒ¼ãƒ«ï¼ˆ23%å‰Šæ¸›ï¼‰ |

### Storage Policy (Auditability)

- Queue items are persisted in the existing `jobs` table with `kind = 'search_queue'`.
  - **Rationale**: The `jobs` table already has priority, state transitions, slot management, and budget tracking. Adding a new table would duplicate schema and audit log management.
  - `input_json` stores the query and search options.
  - `output_json` stores the full result JSON produced by the pipeline execution.
- For **auditability**, completed items store the **full result JSON** (`jobs.output_json`).

### stop_task Semantics (Two Modes)

`stop_task` supports two stop modes:

- **mode=graceful**:
  - Do not start new queued items for the task (queued â†’ cancelled).
  - Wait for running items to complete so their full result JSON can be persisted.
- **mode=immediate**:
  - queued â†’ cancelled.
  - running â†’ cancelled via `asyncio.Task.cancel()`. Result JSON is not persisted.
  - **Batch cancellation**: All running jobs for the task are cancelled at once (`SearchQueueWorkerManager.cancel_jobs_for_task()`).
  - **Worker continuity**: Workers survive individual job cancellations and continue processing other tasks. The worker catches `CancelledError` from the `search_action` task and continues its loop.

**Logical consistency guarantee**: With async I/O and incremental persistence, "immediate" may still leave partial DB artifacts (pages/fragments) written before cancellation. However, **the job's `state = 'cancelled'` in the `jobs` table provides the authoritative record** that the search was cancelled. Downstream consumers can filter out data associated with cancelled jobs.

Cleanup note: For hard cleanup, follow ADR-0005 (Evidence Graph Structure) and use **hard(orphans_only)** after soft cleanup if storage reclamation is required.

### Long Polling Implementation

- Long polling is implemented using **`asyncio.Event`** in `ExplorationState`, not DB polling.
- **Rationale**: `ExplorationState` is already in-memory per task. DB polling adds unnecessary I/O and latency.
- When a search completes, fails, or queue depth changes, the worker calls `state.notify_status_change()` which sets the event.
- `get_status(wait=N)` uses `asyncio.wait_for(state.wait_for_change(), timeout=N)`.
- If timeout expires with no change, the current status is returned (same as `wait=0`).

### Migration Strategy

- Phase 1 completes with `queue_searches` working. **Old tools (`search`, `notify_user`, `wait_for_user`) remain available** during Phase 1.
- Phase 2 removes old tools after confirming `queue_searches` + `get_status(wait)` pattern works in Cursor Rules/Commands.

### Global Rate Limits / Resource Control

When running multiple queue workers, external rate limits must still be respected globally:

| Resource | Control | Status |
|----------|---------|--------|
| **Browser (SERP)** | `BrowserSearchProvider` singleton + `Semaphore(1)` | âœ… Implemented |
| **Academic APIs** | Global rate limiter per provider | ğŸ”œ Phase 4 ([ADR-0013](0013-worker-resource-contention.md)) |
| **HTTP fetch** | `RateLimiter` per domain | âœ… Implemented |

**Note**: Academic API rate limiting is tracked as **Phase 4** in [Q_ASYNC_ARCHITECTURE.md](../Q_ASYNC_ARCHITECTURE.md). See [ADR-0013](0013-worker-resource-contention.md) for design details.

## References
- `docs/Q_ASYNC_ARCHITECTURE.md` - éåŒæœŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°è¨­è¨ˆ
- [ADR-0013: Worker Resource Contention Control](0013-worker-resource-contention.md) - Phase 4 ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆåˆ¶å¾¡
- `src/mcp/server.py` - MCPãƒ„ãƒ¼ãƒ«å®šç¾©
- `src/research/executor.py` - æ¤œç´¢å®Ÿè¡Œ
- `src/research/pipeline.py` - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- `src/scheduler/jobs.py` - ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
- `src/scheduler/search_worker.py` - SearchQueueWorkerå®Ÿè£…
