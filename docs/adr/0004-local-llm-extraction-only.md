# ADR-0004: Local LLM for Extraction Only

## Status
Accepted

## Date
2024-XX-XX（プロジェクト開始時）

## Context

Lyraは複数のMLタスクを実行する必要がある：

| タスク | 目的 |
|--------|------|
| 事実/主張抽出 | ページから引用可能な事実を抽出 |
| 固有表現認識 | エンティティの識別 |
| 品質判定 | AI生成/SEOスパムの検出 |
| 翻訳 | 言語横断ミラークエリ生成 |

これらにはLLMが有効だが、ADR-0002（Zero OpEx）の制約により商用API（GPT-4、Claude API）は使用できない。

一方、ADR-0001で規定した通り、**戦略的判断（クエリ設計、探索方針）はMCPクライアントが担当**する。ローカルLLMがこれらを担当すると品質が低下する。

## Decision

**ローカルLLM（Qwen2.5-3B）を採用し、用途を機械的処理に限定する。**

### モデル選定

| モデル | パラメータ | VRAM | ライセンス | 判定 |
|--------|-----------|------|----------|------|
| Qwen2.5-3B | 3B | ~4GB | Qwen Research* | デフォルト採用 |
| Qwen2.5-7B | 7B | ~8GB | Apache-2.0 | 商用利用時 |
| Llama3.2-3B | 3B | ~4GB | Llama License | 代替 |

*Qwen2.5-3Bは非商用ライセンス。商用利用時は7BまたはLlamaを推奨。

### 許可される用途

| 用途 | 詳細 |
|------|------|
| 事実/主張抽出 | ページテキストから構造化情報を抽出 |
| 固有表現認識 | 人名、組織名、薬剤名等の識別 |
| 言語横断翻訳 | 日→英、英→日のミラークエリ生成 |
| コンテンツ品質判定 | AI生成/アグリゲータ/SEOスパムの判定 |
| 構造化・コンテキスト付与 | 見出し階層構築、断片への文脈情報付与 |

### 禁止される用途

| 用途 | 理由 |
|------|------|
| 検索クエリ設計 | MCPクライアントの専権（ADR-0001） |
| 探索戦略判断 | 推論品質不足 |
| 優先度決定 | 推論品質不足 |
| レポート論理構成 | MCPクライアントの専権 |
| 候補提案 | 責任境界の曖昧化を防止 |

### 実行環境

| 項目 | 設定 |
|------|------|
| ランタイム | Ollama（Podmanコンテナ） |
| ネットワーク | 内部専用（L1セキュリティ層） |
| VRAM管理 | マイクロバッチ + シーケンス長自動短縮で≤8GB |
| フォールバック | GPU失敗時はCPU即時切替 |

## Consequences

### Positive
- **Zero OpEx維持**: API費用なし
- **プライバシー保証**: データがローカルに留まる
- **低レイテンシ**: ネットワーク往復なし
- **オフライン動作**: ネットワーク不要（モデルダウンロード後）

### Negative
- **品質制限**: 3Bモデルは複雑な推論に不向き
- **ハードウェア要件**: GPU推奨（CPU可だが遅い）
- **用途制限**: 戦略的判断には使用不可
- **モデル更新**: 手動でのモデル差し替えが必要

## Alternatives Considered

| Alternative | Pros | Cons | 判定 |
|-------------|------|------|------|
| 商用API（GPT-4） | 高品質 | Zero OpEx違反、データ流出 | 却下 |
| 大型ローカルLLM（70B） | 高品質 | VRAM不足（8GB制約） | 却下 |
| LLMなし（ルールベース） | シンプル | 抽出品質低下 | 却下 |
| 用途制限なし | 柔軟 | 品質低下、責任境界曖昧 | 却下 |

## References
- `docs/archive/REQUIREMENTS.md` §2.1.4（アーカイブ）
- `README.md` "Why Local LLM?"
- `src/filter/llm.py` - LLM統合実装
- `config/settings.yaml` - モデル設定
