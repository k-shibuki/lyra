# ADR-0001: Local-First Architecture / Zero OpEx

## Date
2025-11-01

## Context

研究支援ツールは長期間使用される。商用クラウドAPIへの依存は以下のリスクをもたらす：

| リスク | 詳細 |
|--------|------|
| コスト増大 | API使用量に比例して課金（月額数百〜数千ドル） |
| サービス終了 | APIプロバイダの方針変更でツールが使用不能に |
| レート制限 | 大量の学術論文処理時にスロットリング |
| プライバシー | 研究データが外部サーバーに送信される |
| オフライン不可 | ネットワーク障害時に完全に機能停止 |

一方、ローカル実行には課題もある：

| 課題 | 対策 |
|------|------|
| 計算リソース | 3B程度の小型モデルで十分な性能を確保 |
| セットアップ複雑性 | Ollama + uvによる簡易インストール |
| モデル品質 | 抽出タスクに特化することで品質担保 |

## Decision

**すべての処理をローカルで完結させ、運用コスト（OpEx）をゼロにする。**

具体的には：
1. **LLM処理**: Ollama経由でローカルモデル（Qwen2.5-3B等）を使用
2. **ベクトル検索**: ローカルembeddingモデル + SQLite FTS
3. **Webクローリング**: Playwright（ローカル実行）
4. **データ保存**: SQLite（ローカルファイル）

### 例外（許容する外部通信）
- 学術API（Semantic Scholar、OpenAlex）: 無料・レート制限緩い
- 対象Webサイトへのアクセス: クローリング対象として必須
- Ollamaモデルダウンロード: 初回セットアップのみ

### 禁止する外部依存
- OpenAI API / Anthropic API（有料）
- Google Cloud / AWS / Azure（課金発生）
- 有料CAPTCHA解決サービス
- SaaSベースのベクトルDB（Pinecone等）

## Consequences

### Positive
- **運用コストゼロ**: 電気代以外の継続コストなし
- **完全なデータ主権**: 研究データが外部に出ない
- **オフライン動作**: ネットワーク障害時も過去データで作業可能
- **長期安定性**: 外部サービス終了の影響を受けない

### Negative
- **初期セットアップ**: GPU環境の構築が必要
- **モデル品質の制約**: GPT-4/Claude相当の推論は不可
- **ストレージ消費**: モデルファイルで数GB必要

### 設計への影響
- LLMは「抽出」に特化し、「推論」はMCPクライアントに委譲（ADR-0002参照）
- 認証が必要なサイトはHuman-in-the-Loop方式（ADR-0007参照）

## Alternatives Considered

| Alternative | Pros | Cons | 判定 |
|-------------|------|------|------|
| OpenAI API使用 | 高品質、簡単 | 月額コスト、データ外部送信 | 却下 |
| ハイブリッド（ローカル+API） | 柔軟性 | コスト発生、複雑性 | 却下 |
| セルフホストクラウド | スケーラブル | インフラ運用コスト | 却下 |

## References
- `docs/REQUIREMENTS.md` 1.1節（アーカイブ）
- Ollama: https://ollama.ai
- Qwen2.5: https://huggingface.co/Qwen
