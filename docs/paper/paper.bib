@inproceedings{barbaresiTrafilaturaWebScraping2021,
  title = {Trafilatura: {{A Web Scraping Library}} and {{Command-Line Tool}} for {{Text Discovery}} and {{Extraction}}},
  shorttitle = {Trafilatura},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Barbaresi, Adrien},
  editor = {Ji, Heng and Park, Jong C. and Xia, Rui},
  date = {2021-08},
  pages = {122--131},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-demo.15},
  url = {https://aclanthology.org/2021.acl-demo.15/},
  urldate = {2026-01-04},
  abstract = {An essential operation in web corpus construction consists in retaining the desired content while discarding the rest. Another challenge finding one's way through websites. This article introduces a text discovery and extraction tool published under open-source license. Its installation and use is straightforward, notably from Python and on the command-line. The software allows for main text, comments and metadata extraction, while also providing building blocks for web crawling tasks. A comparative evaluation on real-world data also shows its interest as well as the performance of other available solutions. The contributions of this paper are threefold: it references the software, features a benchmark, and provides a meaningful baseline for similar tasks. The tool performs significantly better than other open-source solutions in this evaluation and in external benchmarks.}
}

@online{bowmanLargeAnnotatedCorpus2015,
  title = {A Large Annotated Corpus for Learning Natural Language Inference},
  author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  date = {2015-08-21},
  eprint = {1508.05326},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1508.05326},
  url = {http://arxiv.org/abs/1508.05326},
  urldate = {2026-01-04},
  abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language}
}

@software{chaseLangChain2022,
  title = {{{LangChain}}},
  author = {Chase, Harrison},
  date = {2022},
  url = {https://github.com/langchain-ai/langchain},
  urldate = {2026-01-04},
  abstract = {ðŸ¦œðŸ”— The platform for reliable agents.}
}

@online{chenM3EmbeddingMultiLingualityMultiFunctionality2024,
  title = {M3-{{Embedding}}: {{Multi-Linguality}}, {{Multi-Functionality}}, {{Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}}},
  shorttitle = {M3-{{Embedding}}},
  author = {Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  date = {2024-02-05},
  url = {https://arxiv.org/abs/2402.03216v5},
  urldate = {2026-01-04},
  abstract = {In this paper, we introduce a new embedding model called M3-Embedding, which is distinguished for its versatility in \textbackslash textit\{Multi-Linguality\}, \textbackslash textit\{Multi-Functionality\}, and \textbackslash textit\{Multi-Granularity\}. It provides a uniform support for the semantic retrieval of more than 100 working languages. It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval. Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens. The effective training of M3-Embedding presents a series of technical contributions. Notably, we propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strategy, which enables a large batch size and high training throughput to improve the discriminativeness of embeddings. M3-Embedding exhibits a superior performance in our experiment, leading to new state-of-the-art results on multilingual, cross-lingual, and long-document retrieval benchmarks.},
  langid = {english}
}

@online{ElicitAIScientific,
  title = {Elicit: {{AI}} for Scientific Research},
  shorttitle = {Elicit},
  author = {{Elicit}},
  date = {2021},
  url = {https://elicit.com/welcome},
  urldate = {2026-01-04},
  abstract = {Use AI to search, summarize, extract data from, and chat with over 125 million papers. Used by over 2 million researchers in academia and industry.},
  langid = {english}
}

@online{heDeBERTaDecodingenhancedBERT2021,
  title = {{{DeBERTa}}: {{Decoding-enhanced BERT}} with {{Disentangled Attention}}},
  shorttitle = {{{DeBERTa}}},
  author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  date = {2021-10-06},
  eprint = {2006.03654},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.03654},
  url = {http://arxiv.org/abs/2006.03654},
  urldate = {2026-01-04},
  abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@online{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2026-01-04},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@software{liuLlamaIndex2022,
  title = {{{LlamaIndex}}},
  author = {Liu, Jerry},
  date = {2022-01-01},
  doi = {10.5281/zenodo.1234},
  url = {https://github.com/jerryjliu/llama_index},
  urldate = {2026-01-04},
  abstract = {LlamaIndex is the leading framework for building LLM-powered agents over your data.}
}

@software{MicrosoftPlaywright2019,
  title = {Playwright},
  author = {{Microsoft}},
  date = {2019},
  url = {https://github.com/microsoft/playwright},
  urldate = {2026-01-04},
  abstract = {Playwright is a framework for Web Testing and Automation. It allows testing Chromium, Firefox and WebKit with a single API.}
}

@software{OllamaOllama2023,
  title = {Ollama},
  author = {{Ollama}},
  date = {2023},
  url = {https://github.com/ollama/ollama},
  urldate = {2026-01-04},
  abstract = {Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.}
}

@online{Perplexity,
  title = {Perplexity},
  author = {{Perplexity AI}},
  date = {2022},
  url = {https://www.perplexity.ai/},
  urldate = {2026-01-04},
  abstract = {Perplexity is a free AI-powered answer engine that provides accurate, trusted, and real-time answers to any question.},
  langid = {english}
}

@online{priemOpenAlexFullyopenIndex2022,
  title = {{{OpenAlex}}: {{A}} Fully-Open Index of Scholarly Works, Authors, Venues, Institutions, and Concepts},
  shorttitle = {{{OpenAlex}}},
  author = {Priem, Jason and Piwowar, Heather and Orr, Richard},
  date = {2022-06-17},
  eprint = {2205.01833},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.01833},
  url = {http://arxiv.org/abs/2205.01833},
  urldate = {2026-01-04},
  abstract = {OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.},
  pubstate = {prepublished},
  keywords = {Computer Science - Digital Libraries}
}

@online{qwenQwen25TechnicalReport2025,
  title = {Qwen2.5 {{Technical Report}}},
  author = {Qwen and Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
  date = {2025-01-03},
  eprint = {2412.15115},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.15115},
  url = {http://arxiv.org/abs/2412.15115},
  urldate = {2026-01-04},
  abstract = {In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language}
}

@software{SeleniumHQSelenium2013,
  title = {Selenium},
  author = {{Selenium}},
  date = {2013},
  url = {https://github.com/SeleniumHQ/selenium},
  urldate = {2026-01-04},
  abstract = {A browser automation framework and ecosystem.}
}

@software{shibukiLyra2026,
  title = {Lyra: {{A Local-First MCP Server}} for {{AI-Collaborative Desktop Research}}},
  author = {Shibuki, Katsuya},
  date = {2026},
  doi = {10.5281/zenodo.18218530},
  url = {https://github.com/k-shibuki/lyra},
  urldate = {2026-01-12},
  abstract = {An open-source MCP server that enables AI assistants to conduct desktop research with evidence graph construction.}
}

@online{SemanticScholarAcademic,
  title = {Semantic {{Scholar Academic Graph API}}},
  author = {{Allen Institute for AI}},
  date = {2025},
  url = {https://www.semanticscholar.org/product/api},
  urldate = {2026-01-04},
  abstract = {Build projects that accelerate scientific progress with the Semantic Scholar Academic Graph API},
  langid = {english}
}

@online{WhatModelContext,
  title = {What Is the {{Model Context Protocol}} ({{MCP}})?},
  author = {{Anthropic}},
  date = {2024},
  url = {https://modelcontextprotocol.io/docs/getting-started/intro},
  urldate = {2026-01-04},
  langid = {english}
}

@inproceedings{wolfTransformersStateoftheArtNatural2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and family=Platen, given=Patrick, prefix=von, useprefix=true and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  editor = {Liu, Qun and Schlangen, David},
  date = {2020-10},
  pages = {38--45},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  url = {https://aclanthology.org/2020.emnlp-demos.6/},
  urldate = {2026-01-04},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.}
}
