> **About this document**
>
> This is an example **Custom Instruction / Skill / Cursor command** for MCP clients: a reusable prompt template defining how an AI assistant orchestrates Lyra MCP tools.
>
> **Cursor commands**: Place `.md` files in `.cursor/commands/` and invoke with `/command-name` in chat.
> See [Cursor Commands documentation](https://cursor.com/docs/agent/chat/commands) for details.
>
> **Claude Desktop skills**: Add via Settings → Skills. Skills work alongside MCP server config.

---

# navigate

## Purpose

Build an evidence graph iteratively using Lyra MCP tools, then synthesize a traceable research report.

## When to use

- Answering evidence-based questions ("Is X effective?", "Does Y cause Z?")
- Comparing alternatives with structured evidence
- Any research requiring source traceability and confidence scoring

---

## Available Tools

### Lyra MCP Tools (evidence collection)
- **Core**: `create_task`, `queue_targets`, `get_status`, `stop_task`
- **Citation Chasing**: `queue_reference_candidates` (explicit control over which references to fetch)
- **Explore**: `vector_search`, `query_view`, `query_sql`, `list_views`
- **Auth**: `get_auth_queue`, `resolve_auth`
- **Admin**: `feedback`, `calibration_metrics`, `calibration_rollback`

### Non-MCP Tools (analysis & exploration)
- **Web search**: Background research, terminology, query brainstorming
- **Browser**: Navigate to URLs, take screenshots, read full-text sources
- **File write**: Export final report to `docs/reports/`

**Key insight**: Combine Lyra (systematic evidence) with non-MCP tools (flexible exploration) for best results.

---

## Contract: Responsibilities & Boundaries

This command operationalizes a 3-layer collaboration model:

| Layer | Actor | Role |
|-------|-------|------|
| **Thinking** | Human | Read primary sources, make final calls, domain expertise |
| **Reasoning** | AI (you) | Design queries, decide next actions, write report |
| **Working** | Lyra | Execute search→fetch→extract→NLI→store; report metrics/materials |

### Responsibility matrix (non-negotiable)

| Responsibility | AI | Lyra |
|---|---|---|
| Search query design / prioritization | ✅ (exclusive) | ❌ (no suggestions) |
| Execute searches / pipeline | ❌ | ✅ |
| Recommend "what to do next" | ✅ | ❌ (metrics only) |
| Evidence graph exploration | ✅ (drive) | ✅ (serve data) |
| Primary-source reading / interpretation | ✅ assist + Human decides | ❌ |

**Key principle**: Lyra is a *navigation tool*. It discovers and organizes sources; detailed analysis is the researcher's role. Lyra never suggests queries or next steps—that is your job.

### Terminology

| Term | Definition |
|------|------------|
| `task_id` | Unique identifier for a task; hypothesis is fixed at creation |
| `hypothesis` | Central claim bound to task_id; immutable after `create_task(hypothesis=...)` |
| `target` | A query or URL object submitted via `queue_targets(...)` — **this is what you optimize** |
| `claim` | Extracted assertion stored in Lyra's evidence graph |

### Bayesian confidence (`bayesian_truth_confidence`)

- **0.5** = no evidence yet (prior)
- **> 0.5** = more support; **< 0.5** = more refutation
- Only **cross-source** NLI edges update confidence (self-citation ignored)
- Abbreviated as "conf" in reports

### Multi-task operation (optional)

You may use **multiple tasks** in a single research session. Useful patterns:
- Separate tasks for supporting vs refuting evidence
- Sub-hypothesis tasks for complex questions
- Exploratory task → focused verification task

**Rule**: Track all `task_id`s used and list them in the final report.

---

## Workflow Overview

```
┌─────────────────────────────────────────────────────────────────┐
│  Search Phase 1: Scout (Web search)                             │
│  → Explore terminology, gather context, design initial query    │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│  Search Phase 2: Evidence (Lyra)                                │
│  → Start with 1 query → review results → design full target set │
│  → Full deployment → wait for all targets to complete           │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│  Search Phase 2.5: Citation Chasing (Optional)                  │
│  → Query v_reference_candidates for unfetched citations         │
│  → Select relevant references → queue URLs via queue_targets    │
│  → Wait for completion → repeat if new references appear        │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│  Search Phase 3: Deep Dive (Browser/Web search integration)     │
│  → Read primary sources as needed                               │
│  → If discoveries emerge → return to Phase 1 to grow graph      │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│  Report                                                         │
│  → Cite ONLY Phase 2/2.5 evidence                               │
│  → Phase 3 info adds context/detail only                        │
│  → stop_task                                                    │
└─────────────────────────────────────────────────────────────────┘
```

---

## Evidence Sources (do not mix in citations)

| Source | Phase | What it is | Allowed use |
|---|---|---|---|
| **Lyra Evidence Graph** | Phase 2 | Traceable chain: claim→fragment→page→URL | **Only basis for citations** in report |
| **Web search** | Phase 1/3 | Background, terminology, query brainstorming | Query design, gap identification; **never as direct evidence** |
| **Full-text reading (browser)** | Phase 3 | Methods/results/limitations notes | Deep analysis of key sources; adds context to Phase 2 claims |

### Traceability rule (hard)

Any claim in the final report **MUST** be backed by Lyra evidence (a URL reachable from `v_evidence_chain` or SQL).

---

## Search Phase 1: Scout

**Goal**: Explore the topic, gather terminology, and design an effective initial query.

### 1.1 Create Task

**New task**: Create one with a testable hypothesis:
```
create_task(hypothesis="{testable_hypothesis}")
→ record task_id
```

**Existing task**: Retrieve task_id and check status:
```
get_status(task_id=task_id, wait=0)
```

The hypothesis is fixed once `create_task` is called. From here, focus on **query optimization**.

### 1.2 Web Search Exploration

Use **web search** (non-MCP tool) to:
- Understand domain terminology
- Identify key papers, authors, concepts
- Find both supporting and refuting perspectives (ADR-0017)
- Brainstorm effective search queries

**Output**: A single, well-crafted initial query for Phase 2.

---

## Search Phase 2: Evidence

**Goal**: Build the traceable evidence graph systematically.

### 2.1 Initial Query (Probe)

Start with **one query** to calibrate:

```
queue_targets(task_id=task_id, targets=[
  {"kind": "query", "query": "{initial_query_from_phase_1}"}
])

get_status(task_id=task_id, wait=60)
```

Review results:
- Check `harvest_rate` — is this query productive?
- Check `metrics.total_claims` — are claims being extracted?
- Use `query_view(view_name="v_claim_evidence_summary", task_id=task_id)` to inspect quality

### 2.2 Design Full Target Set

Based on initial results, design a comprehensive target set:

**Required query categories** (ensure coverage):
1. **Core evidence**: `{core_concept} meta-analysis`, `{core_concept} systematic review`
2. **Mechanism**: `{mechanism} evidence`, `{mechanism} study`
3. **Refutation** (ADR-0017): `{hypothesis} limitations`, `{hypothesis} criticism`
4. **Alternative perspectives**: `{alternative_viewpoint}`, `{competing_hypothesis}`
5. **Gap-filling**: Specific queries addressing gaps from Phase 2.1

**Before deploying, validate the target set**:
- Does it cover both supporting AND refuting angles?
- Are there obvious gaps given the hypothesis?
- Is each query distinct (minimal overlap)?

If validation reveals gaps, add queries. If queries seem redundant, consolidate.

```
queue_targets(task_id=task_id, targets=[
  {"kind": "query", "query": "{query_1}"},
  {"kind": "query", "query": "{query_2}"},
  ...
])
```

**Note**: `targets` can be search queries (strings) or direct URLs (starting with `http://` or `https://`).

### 2.3 Full Deployment & Wait

Wait for all targets to complete:

```
get_status(task_id=task_id, wait=180)  # wait: adjust 30-300 as needed
```

Monitor progress:
- `targets[].satisfaction_score` — 0.7+ indicates good coverage
- `targets[].harvest_rate` — ratio of useful fragments
- `budget.remaining_percent` — resource usage

**If auth blocked**: 
```
get_auth_queue(task_id=task_id)
→ user solves CAPTCHA in browser
→ resolve_auth(action="complete", target="domain", domain="...")
```

Other domains continue while one is blocked—don't stop prematurely.

### 2.4 Review Evidence Graph

Once all targets complete, analyze the collected evidence:

```
query_view(view_name="v_claim_evidence_summary", task_id=task_id)
query_view(view_name="v_contradictions", task_id=task_id)
query_view(view_name="v_unsupported_claims", task_id=task_id)
```

This review informs Phase 2.5 (Citation Chasing) and Phase 3 exploration.

---

## Search Phase 2.5: Citation Chasing (Optional)

**Goal**: Acquire important references found in the References sections of fetched pages.

Citation Chasing is useful when:
- Meta-analyses and systematic reviews reference key primary studies
- Important papers are mentioned but not yet in the evidence graph
- You want deeper coverage of the citation network

### 2.5.1 Check Readiness

Before querying reference candidates, ensure the citation graph is stable:

```
get_status(task_id=task_id, wait=0)
→ check milestones.citation_candidates_stable == true
```

If `citation_candidates_stable: false`, wait for `blockers` to clear:
- `target_queue` — searches still running
- `citation_graph` — citation graph jobs still processing
- `pending_auth` — auth items blocking fetches

### 2.5.2 Query Reference Candidates

Check for unfetched citations:

```
query_view(view_name="v_reference_candidates", task_id=task_id, limit=20)
```

This returns pages cited by task's pages but not yet processed for this task:
- `citation_edge_id` — ID for explicit selection
- `candidate_url` — URL to fetch
- `candidate_domain` — Domain of the candidate
- `citation_context` — Text context where citation was found
- `citing_page_url` — Page that cited this reference

### 2.5.3 Select and Queue References

**Option A: Using `queue_reference_candidates` (recommended)**

Explicit control with minimal effort:

```
# Whitelist mode: only fetch these specific candidates
queue_reference_candidates(task_id=task_id, include_ids=["edge_id_1", "edge_id_2"])

# Blacklist mode: fetch all EXCEPT these
queue_reference_candidates(task_id=task_id, exclude_ids=["edge_id_3"])

# Dry run to preview
queue_reference_candidates(task_id=task_id, limit=10, dry_run=true)
```

DOI URLs are automatically routed to Academic API for faster abstract-only ingestion.

**Option B: Using `queue_targets` directly**

For fine-grained control or mixed target types:

```
queue_targets(task_id=task_id, targets=[
  {"kind": "url", "url": "https://example.com/paper1", "reason": "citation_chase"},
  {"kind": "doi", "doi": "10.1234/example", "reason": "citation_chase"},
  ...
])
```

**Selection criteria**:
- Prioritize primary studies and meta-analyses
- Focus on papers from authoritative domains (pubmed.ncbi, doi.org, arxiv.org)
- Consider citation context — is this likely relevant to your hypothesis?
- Use DOI fast path (`kind: "doi"`) when DOI is known for faster ingestion

### 2.5.4 Wait and Iterate

```
get_status(task_id=task_id, wait=120)
→ wait until milestones.citation_candidates_stable == true
```

After completion, check for new reference candidates:
```
query_view(view_name="v_reference_candidates", task_id=task_id, limit=20)
```

**Iteration decision**:
- If high-quality new candidates appear → repeat 2.5.2-2.5.3
- If candidates are diminishing returns → proceed to Phase 3
- Budget consideration: each URL/DOI costs budget_pages

---

## Search Phase 3: Deep Dive

**Goal**: Verify key sources and discover new angles to strengthen the evidence graph.

### 3.1 Primary Source Reading

Use browser tools to read full-text sources:

1. **Select sources**: Pick high-impact papers from evidence chain
   ```
   query_view(view_name="v_source_impact", task_id=task_id, limit=10)
   ```

2. **Browser read**: Navigate to URLs, screenshot key figures/tables
   ```
   browser_navigate(url="{source_url}")
   browser_snapshot()
   browser_take_screenshot()
   ```

3. **Note observations**: Methods, sample size, limitations, generalizability

### 3.2 Additional Web Search

Use web search to:
- Clarify terminology discovered in Phase 2
- Find related concepts not yet captured
- Identify contradicting viewpoints

### 3.3 Loop Back Decision

After Phase 3 exploration, ask: **Did I discover information that should be in the evidence graph but isn't?**

**If YES** — discoveries exist that warrant new queries:
```
→ Return to Phase 1: Scout
   - Formulate new queries based on discoveries
   - Execute Phase 2 with additional queries
   - Grow the evidence graph
```

**If NO** — no new information to add:
```
→ Proceed to Report
```

This is a binary decision based on discoveries, not a subjective "is it enough?" judgment.

---

## Report

**Goal**: Synthesize findings with full traceability.

### Extract Evidence

```
query_view(view_name="v_evidence_chain", task_id=task_id)
query_sql(sql="SELECT claim_text, bayesian_truth_confidence FROM claims 
               WHERE task_id='{task_id}' AND bayesian_truth_confidence > 0.6
               ORDER BY bayesian_truth_confidence DESC")
```

### Extract Key Sources

Use metric-based selection (not subjective curation):
```
query_view(view_name="v_source_impact", task_id=task_id, limit=10)
```

`impact_score` = claims_generated + (avg_confidence × claims_generated × 0.5) + (claims_supported × 0.3)

### Evidence Timeline

```
query_view(view_name="v_evidence_timeline", task_id=task_id)
```

Flag if all evidence is old (>5 years).

### Extract References

Complete list of all sources:
```
query_sql(sql="SELECT DISTINCT p.url, p.title
               FROM pages p
               JOIN fragments f ON f.page_id = p.id
               JOIN edges e ON e.source_id = f.id AND e.source_type = 'fragment'
               JOIN claims c ON c.id = e.target_id AND e.target_type = 'claim'
               WHERE c.task_id = '{task_id}'
               ORDER BY p.title")
```

### Write Report

Output path: `docs/reports/{YYYYMMDD_HHMMSS}.md`

```markdown
# Research Report

**Date**: {YYYY-MM-DD HH:MM}  
**Hypothesis**: {hypothesis}

## Verdict

{hypothesis}: **SUPPORTED** / **REFUTED** / **INCONCLUSIVE** (confidence: X.XX)

## Key Findings

### Supporting Evidence (Phase 2 — Lyra)
- [Claim] (conf: X.XX) — Source: [Title](URL)

### Refuting Evidence (Phase 2 — Lyra)
- [Claim] (conf: X.XX) — Source: [Title](URL)

## Unresolved Contradictions
- {contradiction_summary}

## Evidence Quality Assessment

### Temporal Distribution
| Year Range | Claims |
|------------|--------|
| 2020-2025  | {n}    |
| 2015-2019  | {n}    |
| <2015      | {n}    |

### Warnings
- {n} claims have outdated evidence (>5 years)
- {n} claims have no supporting evidence

## Methodology

| Phase | Description |
|-------|-------------|
| Phase 1 (Scout) | Web search for query design — not cited |
| Phase 2 (Evidence) | All claims above — traceable to URL via Lyra |
| Phase 3 (Deep Dive) | {list of papers read for context} |

### Task Summary

| task_id | hypothesis | queries | pages | claims |
|---------|------------|---------|-------|--------|
| {id}    | {hyp}      | {n}     | {n}   | {n}    |
```

### Finalize

```
stop_task(task_id=task_id, reason="session_completed")
```

**Note on `stop_task`**:
- Default `scope="all_jobs"` cancels all job kinds (target_queue, verify_nli, citation_graph, etc.)
- Use `scope="target_queue_only"` if you want background jobs (NLI verification, citation graph) to complete
- Tasks are always resumable: call `queue_targets` on the same `task_id` to continue
- After server restart, jobs are reset to `failed` and won't auto-resume

---

## Principles

1. **Lyra navigates, you analyze**: Lyra finds and organizes; detailed reading is your job
2. **Seek refutation early**: Include limitation/criticism queries from the start (ADR-0017)
3. **Probe → validate → deploy**: Start with 1 query, design full set, validate coverage, then deploy
4. **Trace everything**: Every claim in report links to a Phase 2 source only
5. **Phase 3 enriches, not replaces**: Deep dive adds context to Phase 2 claims, not new citations
6. **Loop to grow**: Discoveries in Phase 3 feed back into Phase 1→2 to become traceable
7. **Objective termination**: Task ends when all searches complete and report is written — no subjective "is it enough?" criteria

---

## Reference

### SQL Views

Use `list_views()` to discover available views. Key views:

**Core Evidence Views**:
- `v_claim_evidence_summary` — confidence per claim with support/refute counts
- `v_contradictions` — conflicting evidence requiring resolution
- `v_evidence_chain` — full provenance to URL
- `v_unsupported_claims` — claims without supporting evidence

**Source Evaluation Views**:
- `v_source_impact` — ranks sources by knowledge generation + corroboration (recommended)
- `v_source_authority` — ranks by NLI support edges (DEPRECATED for Key Sources)

**Temporal Views**:
- `v_evidence_timeline` — publication year distribution
- `v_emerging_consensus` — claims with growing support trend
- `v_outdated_evidence` — stale evidence needing review

**Citation Network Views** (for advanced analysis):
- `v_citation_flow` — page-to-page citation relationships
- `v_citation_chains` — A→B→C citation paths
- `v_bibliographic_coupling` — papers citing same sources
- `v_reference_candidates` — unfetched citations for Citation Chasing (requires task_id)

### Auth Handling (CAPTCHA / Login)

`get_auth_queue(task_id=task_id)` → user solves in browser → `resolve_auth(action="complete", target="domain", domain="...")`

Other domains continue while one is blocked—don't stop prematurely.

---

## Admin (optional)

Not part of regular workflow. Use when systematic errors are observed.

### Feedback — Quality Corrections

Use `feedback(action=..., ...)` when evidence quality issues are found:
- `edge_correct` — fix wrong NLI label (`edge_id`, `correct_relation`)
- `claim_reject` / `claim_restore` — mark claim invalid/valid (`claim_id`, `reason`)
- `domain_block` / `domain_unblock` — block/restore domain (`domain_pattern`, `reason`)

### Calibration — NLI Model Tuning

- `calibration_metrics` — view NLI calibration stats and Brier scores
- `calibration_rollback` — revert to previous calibration (destructive)

Use after accumulating `edge_correct` samples to assess/adjust model quality.
