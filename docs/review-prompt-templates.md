# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨æ”¹å–„ææ¡ˆ

**ä½œæˆæ—¥:** 2025-12-27
**æ›´æ–°æ—¥:** 2025-12-27ï¼ˆPhase 1-3æ˜ç¢ºåŒ–ï¼‰
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** ç¢ºå®š
**é–¢é€£:** ADR-0006 (8-Layer Security Model), `config/prompts/*.j2`, `src/filter/llm_security.py`

---

## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€Lyraã® **Jinja2ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆ`config/prompts/*.j2`ï¼‰** ã¨ã€å‘¨è¾ºã«æ®‹å­˜ã™ã‚‹ **Pythonã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**ã€ãŠã‚ˆã³ **LLMå‡ºåŠ›ã®ãƒ‘ãƒ¼ã‚¹/ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ–¹å¼** ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ã™ãå®Ÿè£…ã§ãã‚‹æ”¹å–„æ¡ˆã«è½ã¨ã—è¾¼ã‚€ã€‚ä¸»ãªæ‰€è¦‹:

1. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ª:** Aï¼ˆå„ªç§€ï¼‰ã‹ã‚‰Dï¼ˆè¦æ”¹å–„ï¼‰ã¾ã§å¹…ãŒã‚ã‚‹
2. **è¨€èªã®ä¸çµ±ä¸€:** ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé–“ã§æ—¥æœ¬èªã¨è‹±èªãŒæ··åœ¨
3. **å‡ºåŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³:** ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é¢ï¼ˆADR-0006ï¼‰ã¯å …ç‰¢ã ãŒã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¼·åˆ¶ã¯å¼±ã„
4. **ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹:** ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯å­˜åœ¨ã™ã‚‹ãŒã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä»˜ãæ§‹é€ åŒ–ãƒªãƒˆãƒ©ã‚¤ã¯ãªã„
5. **âš ï¸ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤–éƒ¨åŒ–ã®ä¸å¾¹åº•:** ä¸€éƒ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒPythonã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã§æ®‹ã£ã¦ã„ã‚‹ï¼ˆè¨­è¨ˆé•åï¼‰

---

## å‰æï¼ˆæœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å®Ÿè£…æ–¹é‡ï¼‰

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ”¹å–„æ¡ˆã¯ã€ä»¥ä¸‹ã®å‰æã§ã€Œãã®ã¾ã¾å®Ÿè£…ã«ç§»ã›ã‚‹ã€ç²’åº¦ã«ã™ã‚‹ã€‚

- **DBæ–¹é‡**: **DBã¯ä½œã‚Šç›´ã—å‰æ**ï¼ˆ`data/lyra.db` ã‚’ç ´æ£„ã—ã€`src/storage/schema.sql` ã‹ã‚‰å†ç”Ÿæˆï¼‰ã€‚**å¾Œæ–¹äº’æ›æ€§ã¯ä¸€åˆ‡ä¸è¦**ã€‚
  - migration æ©Ÿæ§‹ï¼ˆ`schema_migrations` + `migrations/*.sql`ï¼‰ã¯æ—¢ã«å­˜åœ¨ã™ã‚‹ãŒã€**æœ¬ãƒ•ã‚§ãƒ¼ã‚ºã§ã¯ä½¿ç”¨ã—ãªã„**ï¼ˆæ®‹ã™/æ¶ˆã™ã¯åˆ¥ãƒ•ã‚§ãƒ¼ã‚ºã®åˆ¤æ–­ï¼‰ã€‚
  - äº’æ›æ€§ç¶­æŒã®ãŸã‚ã®åˆ†å²ï¼ˆæ—§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åãƒ»æ—§JSONå½¢å¼ãªã©ï¼‰ã‚’ **ã‚³ãƒ¼ãƒ‰ã«æ®‹ã•ãªã„**ã€‚
- **LLMå‡ºåŠ›ã®æ‰±ã„**: LLMã¯ã€ŒJSON onlyã€ã¨æŒ‡ç¤ºã—ã¦ã‚‚å‰ç½®ãæ–‡å­—åˆ—ã‚„Markdown code fenceã‚’æ··ãœã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ã€**JSONæŠ½å‡ºã¯1ç®‡æ‰€ã«é›†ç´„**ã—ã€å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§åŒã˜ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ã†ã€‚
- **ADRæ•´åˆ**:
  - **ADR-0006**: `validate_llm_output()` ã«ã‚ˆã‚‹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼ˆæ¼æ´©æ¤œçŸ¥/URLæ¤œçŸ¥ç­‰ï¼‰ã‚’ç¶­æŒã€‚
  - ãƒ­ãƒ¼ã‚«ãƒ«LLMåˆ¶ç´„ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³/å‡¦ç†æ¯”ç‡ï¼‰ã‚’ç¶­æŒã€‚
- **å‡ºåŠ›è¨€èª**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬ä½“ãƒ»LLMå‡ºåŠ›ã¨ã‚‚ã« **è‹±èªé™å®š**ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€å¤§åŒ–ã®ãŸã‚ï¼‰ã€‚

## Part 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¸€è¦§

### 1.1 Jinja2ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (`config/prompts/*.j2`)

| ãƒ•ã‚¡ã‚¤ãƒ« | ç”¨é€” | è¨€èª | è©•ä¾¡ | å„ªå…ˆåº¦ |
|------|---------|----------|--------|----------|
| `extract_facts.j2` | å®¢è¦³çš„äº‹å®Ÿã®æŠ½å‡º | JP | C | High |
| `extract_claims.j2` | æ–‡è„ˆä»˜ãä¸»å¼µã®æŠ½å‡º | JP | C | High |
| `summarize.j2` | ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„ | JP | D | Critical |
| `translate.j2` | ç¿»è¨³ | JP | D | Medium |
| `decompose.j2` | åŸå­ä¸»å¼µã¸ã®åˆ†è§£ | JP | B | Low |
| `detect_citation.j2` | å¼•ç”¨ãƒªãƒ³ã‚¯ vs ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯åˆ¤å®š | JP | B | Low |
| `relevance_evaluation.j2` | å¼•ç”¨é–¢é€£åº¦ 0-10 è©•ä¾¡ | JP | A | - |

### 1.2 Python ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆâš ï¸ å¤–éƒ¨åŒ–ãŒå¿…è¦ï¼‰

> **è¨­è¨ˆé•å:** `src/utils/prompt_manager.py` ã¨ `render_prompt()` ã«ã‚ˆã‚Šã€ŒLLMå…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ `config/prompts/*.j2` ã«å¤–éƒ¨åŒ–ã€ã¨ã„ã†æ§‹é€ ãŒæ—¢ã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ãŒã€ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ã§æ®‹ã£ã¦ã„ã‚‹ã€‚Phase 0 ã§å¤–éƒ¨åŒ–ã™ã¹ãã€‚

| ãƒ•ã‚¡ã‚¤ãƒ« | å¤‰æ•°å | ç”¨é€” | è¨€èª | è©•ä¾¡ | å¤–éƒ¨åŒ– | æ”¹å–„æ¡ˆ |
|----------|----------|---------|----------|--------|--------|--------|
| `src/extractor/quality_analyzer.py` | `LLM_QUALITY_ASSESSMENT_PROMPT` | ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªè©•ä¾¡ | EN | B | âš ï¸ è¦å¤–éƒ¨åŒ– | Part 2.9 |
| `src/extractor/quality_analyzer.py` | `LLM_QUALITY_ASSESSMENT_PROMPT_EN` | ä¸Šè¨˜ã¨åŒç­‰ï¼ˆENç‰ˆï¼‰ | EN | B | âš ï¸ è¦å¤–éƒ¨åŒ– | Part 2.9å‚ç…§ |
| `src/report/chain_of_density.py` | `INITIAL_SUMMARY_PROMPT` | CoDåˆæœŸè¦ç´„ | EN | B | âš ï¸ è¦å¤–éƒ¨åŒ– | Part 2.10 |
| `src/report/chain_of_density.py` | `DENSIFY_PROMPT` | CoDé«˜å¯†åº¦åŒ– | EN/JPæ··åœ¨ | C | âš ï¸ è¦å¤–éƒ¨åŒ– | Part 2.8 |
| `src/filter/llm.py` | `EXTRACT_FACTS_INSTRUCTION` | æ¼æ´©æ¤œå‡ºç”¨ (â€»1) | EN | - | ç¶­æŒOK | å¯¾è±¡å¤– |
| `src/filter/llm.py` | `EXTRACT_CLAIMS_INSTRUCTION` | æ¼æ´©æ¤œå‡ºç”¨ (â€»1) | EN | - | ç¶­æŒOK | å¯¾è±¡å¤– |
| `src/filter/llm.py` | `SUMMARIZE_INSTRUCTION` | æ¼æ´©æ¤œå‡ºç”¨ (â€»1) | EN | - | ç¶­æŒOK | å¯¾è±¡å¤– |
| `src/filter/llm.py` | `TRANSLATE_INSTRUCTION` | æ¼æ´©æ¤œå‡ºç”¨ (â€»1) | EN | - | ç¶­æŒOK | å¯¾è±¡å¤– |
| `src/extractor/citation_detector.py` | `_DETECT_CITATION_INSTRUCTIONS` | ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ (â€»2) | JP | - | ç¶­æŒOK | å¯¾è±¡å¤– |

**æ³¨è¨˜:**
- â€»1: **æ¼æ´©æ¤œå‡ºç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ** - LLMå‡ºåŠ›ã«å¯¾ã™ã‚‹n-gramãƒãƒƒãƒãƒ³ã‚°ã§ä½¿ç”¨ï¼ˆADR-0006 L4ï¼‰ã€‚LLMã¸ã®å…¥åŠ›ã§ã¯ãªãã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãŸã‚ã€å¤–éƒ¨åŒ–ä¸è¦ã€‚
- â€»2: **ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨** - `validate_llm_output()` ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ä½¿ç”¨ã€‚å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ `detect_citation.j2` ã«ã‚ã‚‹ã€‚å¤–éƒ¨åŒ–ä¸è¦ã€‚
- **âš ï¸ è¦å¤–éƒ¨åŒ–**: `src/utils/prompt_manager.py` ã®è¨­è¨ˆæ–¹é‡ï¼ˆå¤–éƒ¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåŒ–ï¼‰ã«é•åã€‚`config/prompts/*.j2` ã¸ç§»å‹•ãŒå¿…è¦ã€‚

---

## Part 2: å€‹åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼

### 2.1 `extract_facts.j2` â€” è©•ä¾¡: C

**ç¾çŠ¶:**
```
ã‚ãªãŸã¯æƒ…å ±æŠ½å‡ºã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å®¢è¦³çš„ãªäº‹å®Ÿã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{{ text }}

æŠ½å‡ºã—ãŸäº‹å®Ÿã‚’JSONé…åˆ—å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å„äº‹å®Ÿã¯ä»¥ä¸‹ã®å½¢å¼ã§:
{"fact": "äº‹å®Ÿã®å†…å®¹", "confidence": 0.0-1.0ã®ä¿¡é ¼åº¦}

äº‹å®Ÿã®ã¿ã‚’å‡ºåŠ›ã—ã€æ„è¦‹ã‚„æ¨æ¸¬ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
```

**å•é¡Œç‚¹:**
- ã€Œäº‹å®Ÿã€ã®å®šç¾©ãŒãªã„ï¼ˆæ¤œè¨¼å¯èƒ½ãªè¨˜è¿°ï¼Ÿè¦³å¯Ÿï¼Ÿï¼‰
- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åŸºæº–ãŒãªã„
- å‡ºåŠ›ä»¶æ•°åˆ¶é™ãŒãªã„ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æµªè²»ãƒªã‚¹ã‚¯ï¼‰
- Few-shotä¾‹ãŒãªã„
- ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã®åˆ†é¡ãŒãªã„

**æ”¹å–„æ¡ˆ:**
```jinja2
You are an expert in information extraction for academic research.

## Task
Extract verifiable factual statements from the text below.

## Definition of "Fact"
- Empirically verifiable claims (not opinions or predictions)
- Contains specific entities (names, numbers, dates, locations)
- Can be traced to a primary source

## Input
{{ text }}

## Output Requirements
- Return 3-10 most important facts as JSON array
- Each fact: {"fact": "...", "confidence": 0.0-1.0, "evidence_type": "statistic|citation|observation"}
- Confidence criteria:
  - 1.0: Directly stated with explicit source
  - 0.7-0.9: Stated clearly without source
  - 0.5-0.6: Implied or paraphrased
  - 0.3-0.4: Inferred from context

## Example
[{"fact": "DPP-4 inhibitors reduced HbA1c by 0.5-1.0%", "confidence": 0.9, "evidence_type": "statistic"}]

Output JSON array only:
```

---

### 2.2 `extract_claims.j2` â€” è©•ä¾¡: C

**ç¾çŠ¶:**
```
ã‚ãªãŸã¯æƒ…å ±åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¸»å¼µã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³: {{ context }}

ãƒ†ã‚­ã‚¹ãƒˆ:
{{ text }}

æŠ½å‡ºã—ãŸä¸»å¼µã‚’JSONé…åˆ—å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å„ä¸»å¼µã¯ä»¥ä¸‹ã®å½¢å¼ã§:
{"claim": "ä¸»å¼µã®å†…å®¹", "type": "fact|opinion|prediction", "confidence": 0.0-1.0}
```

**å•é¡Œç‚¹:**
- ãƒªã‚µãƒ¼ãƒã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ï¼ˆ`context`ï¼‰ã®ä½¿ã„æ–¹ãŒä¸æ˜ç¢º
- ä¸»å¼µã‚¿ã‚¤ãƒ—ã®åˆ†é¡ãŒå˜ç´”ã™ãã‚‹ï¼ˆfact/opinion/predictionï¼‰
- ã‚¯ã‚¨ãƒªã¸ã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢ãŒãªã„
- ç²’åº¦ã®æŒ‡å®šãŒãªã„

**æ”¹å–„æ¡ˆ:**
```jinja2
You are a research analyst extracting claims relevant to a specific research question.

## Research Question
{{ context }}

## Source Text
{{ text }}

## Task
Extract claims that directly help answer the research question above.

## Claim Types
- fact: Verifiable statement about current/past state (can be checked)
- opinion: Value judgment or recommendation
- prediction: Future-oriented claim

## Output
JSON array with 1-5 most relevant claims:
{
  "claim": "claim text",
  "type": "fact|opinion|prediction",
  "relevance_to_query": 0.0-1.0,
  "confidence": 0.0-1.0
}

Prioritize claims that:
1. Directly address the research question
2. Contain specific, verifiable information
3. Are supported by evidence in the text

Output JSON array only:
```

---

### 2.3 `summarize.j2` â€” è©•ä¾¡: Dï¼ˆCriticalï¼‰

**ç¾çŠ¶:**
```
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{{ text }}

è¦ç´„:
```

**å•é¡Œç‚¹:**
- æŒ‡ç¤ºãŒæ¥µã‚ã¦æ±ç”¨çš„
- å‡ºåŠ›é•·ã®æŒ‡å®šãŒãªã„
- æ§‹é€ åŒ–å‡ºåŠ›ãŒãªã„
- ç›®çš„ã®æŒ‡å®šãŒãªã„
- ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ä¿æŒã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ãŒãªã„

**æ”¹å–„æ¡ˆ:**
```jinja2
You are a research summarizer for evidence synthesis.

## Input Text
{{ text }}

## Task
Create a concise summary preserving key evidence.

## Requirements
- Length: {{ max_words | default(100) }} words maximum
- Focus: Claims, findings, and their supporting evidence
- Preserve: Specific numbers, dates, source attributions
- Exclude: Background context, methodology details (unless critical)

## Output Format
Summary text only (no JSON, no bullet lists, no headings):
```

---

### 2.4 `translate.j2` â€” è©•ä¾¡: D

**ç¾çŠ¶:**
```
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{{ target_lang }}ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{{ text }}

ç¿»è¨³:
```

**å•é¡Œç‚¹:**
- æŠ€è¡“/åŒ»ç™‚ç”¨èªã®æ‰±ã„ãŒãªã„
- å›ºæœ‰åè©ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ãŒãªã„
- æ•°å€¤ã®ç²¾åº¦è¦ä»¶ãŒãªã„

**æ”¹å–„æ¡ˆ:**
```jinja2
You are a professional translator specializing in academic and medical texts.

## Source Text
{{ text }}

## Target Language
{{ target_lang }}

## Translation Guidelines
- Preserve technical/medical terminology accurately
- Keep proper nouns (drug names, study names) in original form
  - Add translation in parentheses if helpful: "sitagliptin (ã‚·ã‚¿ã‚°ãƒªãƒ—ãƒãƒ³)"
- Maintain numerical precision (doses, percentages, p-values)
- Preserve citation markers [1], [2], etc.
- Do not add or remove information

## Output
Translated text only (no explanations or notes):
```

---

### 2.5 `decompose.j2` â€” è©•ä¾¡: Bï¼ˆè‰¯å¥½ï¼‰

**é•·æ‰€:**
- è©³ç´°ãªã‚¹ã‚­ãƒ¼ãƒå®šç¾©
- Few-shotä¾‹ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹
- æ˜ç¢ºãªåˆ¶ç´„

**è»½å¾®ãªå•é¡Œ:**
- æ—¥æœ¬èªå‡ºåŠ›ãŒãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰
- `hints` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒæ›–æ˜§

**è¿½åŠ ææ¡ˆ:**
```jinja2
{# æ—¢å­˜ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¸ã®è¿½åŠ  #}

## Additional Guidance for hints
hints should specify concrete source types:
- Good: "PubMed RCTs", "FDA approval documents", "Cochrane reviews"
- Bad: "search online", "check news"
```

> **æ³¨:** å‡ºåŠ›è¨€èªã¯è‹±èªå›ºå®šï¼ˆPart 2.11å‚ç…§ï¼‰ã€‚`output_lang` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å°å…¥ã—ãªã„ã€‚

---

### 2.6 `detect_citation.j2` â€” è©•ä¾¡: B

**é•·æ‰€:**
- æ˜ç¢ºãªYES/NOå‡ºåŠ›
- å…·ä½“çš„ãªé™¤å¤–åŸºæº–

**è»½å¾®ãªå•é¡Œ:**
- å­¦è¡“å¼•ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒä¸è¶³

**è¿½åŠ ææ¡ˆ:**
```jinja2
{# æ—¢å­˜åŸºæº–ã¸ã®è¿½åŠ  #}

Academic citation indicators (high confidence):
- DOI links (doi.org/10.xxxx/...)
- PubMed links (pubmed.ncbi.nlm.nih.gov/...)
- arXiv links (arxiv.org/abs/...)
- Reference markers: [1], [2], (Smith et al., 2023)
- Academic phrases: "et al.", "Fig.", "Table", "Supplementary"
```

---

### 2.7 `relevance_evaluation.j2` â€” è©•ä¾¡: Aï¼ˆå„ªç§€ï¼‰

**é•·æ‰€:**
- æ˜ç¢ºãª0-10ã‚¹ã‚±ãƒ¼ãƒ«ã¨å…·ä½“çš„ãªåŸºæº–
- SUPPORTS/REFUTESåˆ¤å®šã®æ˜ç¤ºçš„é™¤å¤–
- ã€Œæœ‰ç”¨æ€§ã€è©•ä¾¡è»¸ã®æ˜ç¢ºãªå®šç¾©

**å¤‰æ›´ä¸è¦ã€‚** ã“ã‚ŒãŒå“è³ªã®å‚ç…§ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€‚

---

### 2.8 `DENSIFY_PROMPT` â€” è©•ä¾¡: C

**å ´æ‰€:** `src/report/chain_of_density.py`

**ç¾çŠ¶:**
```python
DENSIFY_PROMPT = """You are an expert in information compression. Improve the following summary to be more dense.

[Current Summary]
{current_summary}

[Original Information]
{original_content}

[Missing Entities]
{missing_entities}

[Requirements]
1. Include more important information while maintaining summary length
2. Include missing entities as much as possible
3. Preserve source information for each claim
4. Remove redundant expressions and increase information density
5. Maintain approximately 100-150 words

[Output Format]
...
JSONå‡ºåŠ›ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„:"""  # â† è¨€èªæ··åœ¨
```

**å•é¡Œç‚¹:**
- è¨€èªæ··åœ¨ï¼ˆè‹±èªæœ¬æ–‡ + æ—¥æœ¬èªãƒ•ãƒƒã‚¿ãƒ¼ï¼‰
- ã€Œå­¦è¡“ç ”ç©¶æ”¯æ´ã€ã¨ã„ã†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒä¸è¶³
- Evidence Graphã¨ã®é€£æºãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„
- ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®é‡è¦åº¦åŸºæº–ãŒãªã„
- çŸ›ç›¾æ¤œå‡ºã®æŒ‡ç¤ºãŒãªã„

**æ”¹å–„æ¡ˆ:**
```python
DENSIFY_PROMPT = """You are an expert in information compression for academic research synthesis.

## Purpose
Increase information density while preserving evidence quality for claim verification.

## Current Summary
{current_summary}

## Original Information
{original_content}

## Missing Entities (priority order)
{missing_entities}

## Requirements
1. **Density Increase**: Include more verifiable information without increasing length
2. **Entity Integration**: Incorporate missing entities, prioritizing:
   - Quantitative data (numbers, percentages, dates)
   - Named entities (researchers, institutions, studies)
   - Causal relationships
3. **Source Preservation**: Maintain source attribution for each claim
4. **Redundancy Removal**: Eliminate repetitive or vague expressions
5. **Length Constraint**: Maintain approximately 100-150 words
6. **Conflict Detection**: Note if new entities contradict existing claims

## Output Format
{{
  "summary": "densified summary text",
  "entities": ["entity1", "entity2", ...],
  "claims": [
    {{
      "text": "verifiable claim",
      "source_indices": [0, 1],
      "claim_type": "factual|causal|comparative|temporal|quantitative",
      "confidence": 0.0-1.0
    }}
  ],
  "density_metrics": {{
    "entities_added": <number>,
    "entities_total": <number>,
    "compression_ratio": <float>
  }},
  "conflicts": ["any contradictions with existing claims"]
}}

Return only JSON output:"""
```

**Jinja2ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåŒ–æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«å:** `config/prompts/densify.j2`

---

### 2.9 `LLM_QUALITY_ASSESSMENT_PROMPT` â€” è©•ä¾¡: B

**å ´æ‰€:** `src/extractor/quality_analyzer.py`

**ç¾çŠ¶:**
```python
LLM_QUALITY_ASSESSMENT_PROMPT = """You are an expert in web content quality assessment...
Evaluation criteria:
- Does it have unique insights or analysis?
- Is it based on primary sources?
- Is the writing natural and human-like?
- Are ads or affiliate links excessive?
- Is the information accurate and trustworthy?
...
```

**é•·æ‰€:**
- æ˜ç¢ºãªè©•ä¾¡åŸºæº–5é …ç›®
- JSONå‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŒ‡å®š
- æ—¥æœ¬èª/è‹±èªç‰ˆã®ä¸¡æ–¹ãŒå­˜åœ¨

**å•é¡Œç‚¹:**
- ã€ŒLyraç‰¹æœ‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€ãŒä¸è¶³ï¼ˆå­¦è¡“ç ”ç©¶æ”¯æ´ã¨ã„ã†ç›®çš„ï¼‰
- `is_ai_generated` ã®åˆ¤å®šåŸºæº–ãŒæ›–æ˜§
- ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®å“è³ªæŒ‡æ¨™ãŒãªã„ï¼ˆå­¦è¡“ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®é©åˆåº¦ï¼‰

**æ”¹å–„æ¡ˆ:**
```python
LLM_QUALITY_ASSESSMENT_PROMPT = """You are an expert in evaluating web content quality for academic research purposes.

## Context
This content will be used as evidence in a research synthesis system.
Prioritize academic credibility over general web quality.

## Text (first 2000 characters)
{text}

## Evaluation Criteria
1. **Source Authority**: Is this from a primary source, peer-reviewed publication, or authoritative institution?
2. **Evidence Quality**: Does it contain specific data, citations, or verifiable claims?
3. **Originality**: Is this original research/analysis vs. aggregated/summarized content?
4. **Objectivity**: Is the content neutral and evidence-based vs. opinion/promotional?
5. **Recency**: Is the information current and relevant?

## Output Format
{{
  "quality_score": 0.0-1.0,
  "is_ai_generated": true/false,
  "is_spam": true/false,
  "is_aggregator": true/false,
  "academic_relevance": 0.0-1.0,
  "evidence_density": "high|medium|low",
  "reason": "concise explanation"
}}

Respond in JSON only:"""
```

---

### 2.10 `INITIAL_SUMMARY_PROMPT` â€” è©•ä¾¡: B

**å ´æ‰€:** `src/report/chain_of_density.py`

**ç¾çŠ¶:**
```python
INITIAL_SUMMARY_PROMPT = """You are an expert in information summarization...
[Requirements]
1. Extract key facts and claims
2. Preserve source information corresponding to each claim
3. Create a summary of approximately 100-150 words
4. Include important entities (person names, organization names, dates, numbers)
...
```

**é•·æ‰€:**
- Chain-of-Densityã®åˆæœŸè¦ç´„ã¨ã—ã¦é©åˆ‡
- ã‚½ãƒ¼ã‚¹æƒ…å ±ä¿æŒã®è¦ä»¶ã‚ã‚Š
- ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºã®æŒ‡ç¤ºã‚ã‚Š

**å•é¡Œç‚¹:**
- ã€Œå­¦è¡“ç ”ç©¶æ”¯æ´ã€ã¨ã„ã†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒä¸è¶³
- Evidence Graphã¨ã®é€£æºãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„
- ã‚¯ã‚¨ãƒªã¨ã®é–¢é€£åº¦ã‚’è€ƒæ…®ã—ã¦ã„ãªã„

**æ”¹å–„æ¡ˆ:**
```python
INITIAL_SUMMARY_PROMPT = """You are an expert in summarizing research materials for evidence synthesis.

## Purpose
This summary will be used in an evidence graph to support or refute research claims.
Focus on extractable, verifiable information.

## Input Information
{content}

## Research Context (if available)
{query_context}

## Requirements
1. Extract claims that can be independently verified
2. Preserve source attribution for each claim
3. Prioritize quantitative data (statistics, measurements, dates)
4. Create a summary of approximately 100-150 words
5. Flag conflicting or contradictory information

## Output Format
{{
  "summary": "summary text",
  "entities": ["entity1", "entity2", ...],
  "claims": [
    {{
      "text": "verifiable claim",
      "source_indices": [0, 1],
      "claim_type": "factual|causal|comparative|temporal|quantitative",
      "confidence": 0.0-1.0
    }}
  ],
  "conflicts": ["any contradictions noted"]
}}

Return only JSON output:"""
```

---

## Part 2.11: Lyraé©åˆæ€§ã®è€ƒæ…®äº‹é …

### å‡ºåŠ›è¨€èªãƒãƒªã‚·ãƒ¼

**æ–¹é‡**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬ä½“ãƒ»LLMå‡ºåŠ›ã¨ã‚‚ã« **è‹±èªé™å®š**

**ç†ç”±:**
- ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆOllama/Qwenï¼‰ã¯è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ–¹ãŒæ€§èƒ½ãŒè‰¯ã„
- å‡ºåŠ›ã®ä¸€è²«æ€§ã¨ãƒ‘ãƒ¼ã‚¹å®¹æ˜“æ€§ã‚’ç¢ºä¿
- æ—¥æœ¬èªãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã®ç¿»è¨³ã¯åˆ¥ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆMCP Clientå´ï¼‰ã§å¯¾å¿œ

**å®Ÿè£…:**
- å…¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è‹±èªåŒ–ï¼ˆPhase 1ã§å®Ÿæ–½ï¼‰
- `output_lang` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å°å…¥ã—ãªã„
- Few-shotä¾‹ã‚‚è‹±èªã§çµ±ä¸€

### ClaimTypeæ•´åˆæ€§

**é‡è¦:** Lyraã«ã¯ã€ŒClaimTypeã€ãŒè¤‡æ•°ã®æ–‡è„ˆã§ç™»å ´ã™ã‚‹ãŸã‚ã€æ··åŒã—ãªã„ã€‚

- **A. Claim Decompositionï¼ˆç ”ç©¶ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³åˆ†è§£ï¼‰**: `src/filter/claim_decomposition.py:ClaimType`
  - ç›®çš„: ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ã‚’ *æ¤œè¨¼å¯èƒ½ãªåŸå­ä¸»å¼µ* ã«åˆ†è§£ã™ã‚‹éš›ã®åˆ†é¡ï¼ˆ`factual|causal|comparative|definitional|temporal|quantitative`ï¼‰
  - ã“ã‚Œã¯ **extract_claims ã®åˆ†é¡ï¼ˆDBä¿å­˜/ãƒ¬ãƒãƒ¼ãƒˆåˆ†é¡ï¼‰ã¨ã¯åˆ¥æ¦‚å¿µ**
- **B. Extract Claimsï¼ˆãƒšãƒ¼ã‚¸/æ–­ç‰‡ã‹ã‚‰ã®ä¸»å¼µæŠ½å‡ºï¼‰**: `config/prompts/extract_claims.j2` ã® `"type"`
  - ç›®çš„: DB `claims.claim_type` ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®ç°¡æ˜“åˆ†é¡ï¼ˆä¾‹: `fact|opinion|prediction`ï¼‰

**çµè«–ï¼ˆPhase 1ã€œ2ã®æ–¹é‡ï¼‰**:

- `extract_claims.j2` ã¯å½“é¢ **`type: "fact|opinion|prediction"` ã‚’ç¶­æŒ**ã—ã€å¿…è¦ãªã‚‰ `relevance_to_query` ç­‰ã‚’è¿½åŠ ã™ã‚‹ã€‚
- `claim_decomposition.py:ClaimType` ã« `predictive/normative` ã‚’ç„¡ç†ã«è¿½åŠ ã—ãªã„ï¼ˆçµ±åˆå†è¨­è¨ˆã¯åˆ¥ãƒ•ã‚§ãƒ¼ã‚ºï¼‰ã€‚

### ãƒ­ãƒ¼ã‚«ãƒ«LLMåˆ¶ç´„ï¼ˆADR-0004ï¼‰

**è€ƒæ…®äº‹é …:**
- Ollamaä½¿ç”¨ã«ã‚ˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
- è¤‡é›‘ã™ãã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯æ€§èƒ½ä½ä¸‹
- Few-shotä¾‹ã®è¿½åŠ ã¯ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»å¢—

**æ¨å¥¨:**
1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯300-500ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å†…ã‚’ç›®æ¨™
2. Few-shotä¾‹ã¯1ã¤ã«é™å®š
3. è¤‡é›‘ãªã‚¹ã‚­ãƒ¼ãƒã‚ˆã‚Šå˜ç´”ãªæŒ‡ç¤ºã‚’å„ªå…ˆ

---

## Part 3: å‡ºåŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ

### 3.1 ç¾åœ¨ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿæ§‹

| ãƒ¬ã‚¤ãƒ¤ãƒ¼ | æ©Ÿæ§‹ | å ´æ‰€ | ã‚«ãƒãƒ¬ãƒƒã‚¸ |
|-------|-----------|----------|----------|
| **L2** | å…¥åŠ›ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ | `llm_security.py:sanitize_llm_input()` | å…¨LLMå…¥åŠ› |
| **L3** | ã‚·ã‚¹ãƒ†ãƒ ã‚¿ã‚°ä¿è­· | `llm_security.py:generate_session_tag()` | ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ |
| **L4** | å‡ºåŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ | `llm_security.py:validate_llm_output()` | å…¨LLMå‡ºåŠ› |
| **L7** | ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ | `mcp/response_sanitizer.py` | MCPãƒ¬ã‚¹ãƒãƒ³ã‚¹ |

### 3.2 JSONè§£æãƒ‘ã‚¿ãƒ¼ãƒ³

**ç¾åœ¨ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆå…¨ç®‡æ‰€å…±é€šï¼‰:**
```python
# ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹å…¨ä½“ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
try:
    json_match = re.search(r"\[.*\]", response, re.DOTALL)  # or r"\{.*\}"
    if json_match:
        parsed = json.loads(json_match.group())
    else:
        parsed = []  # or {}
except json.JSONDecodeError:
    parsed = fallback_value
```

**ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«:**
- `src/filter/llm.py`
- `src/filter/claim_decomposition.py`
- `src/report/chain_of_density.py`
- `src/extractor/quality_analyzer.py`

### 3.3 æ•°å€¤ã‚¹ã‚³ã‚¢ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

**0-10ã‚¹ã‚³ã‚¢ (relevance_evaluation):**
```python
# src/search/citation_filter.py:_parse_llm_score_0_10()
def _parse_llm_score_0_10(text: str) -> int | None:
    m = _INT_RE.search(text.strip())
    if not m:
        return None
    n = int(m.group(1))
    return max(0, min(10, n))  # [0, 10]ã«ã‚¯ãƒ©ãƒ³ãƒ—
```

**0.0-1.0ã‚¹ã‚³ã‚¢ (quality, confidence):**
```python
# å…¨ä½“ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¯ãƒ©ãƒ³ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³
score = max(0.0, min(1.0, raw_score))
```

### 3.4 YES/NOæ­£è¦åŒ–

```python
# src/extractor/citation_detector.py:_normalize_yes_no()
def _normalize_yes_no(text: str) -> str | None:
    cleaned = text.strip().upper()
    cleaned = re.sub(r"[^A-Z]", "", cleaned)
    if cleaned.startswith("YES"):
        return "YES"
    if cleaned.startswith("NO"):
        return "NO"
    return None
```

### 3.5 ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ | å ´æ‰€ |
|-----------|------------------|----------|
| Claim Decomposition | ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ | `claim_decomposition.py:_decompose_with_rules()` |
| Chain-of-Density | ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åœ§ç¸® | `chain_of_density.py` |
| Quality Assessment | `None`ã‚’è¿”ã—ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ | `quality_analyzer.py` |
| Citation Detection | `is_citation=False`ã‚’è¿”ã™ | `citation_detector.py` |

---

## Part 4: ã‚®ãƒ£ãƒƒãƒ—ã¨æ”¹å–„ææ¡ˆ

### 4.1 ä¸è¶³: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä»˜ãæ§‹é€ åŒ–ãƒªãƒˆãƒ©ã‚¤

**ç¾çŠ¶:** ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã€å³åº§ã«ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚

**å•é¡Œ:** LLMãŒè»½å¾®ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå•é¡Œã§æ­£ã—ã„å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

**ææ¡ˆ: ä¿®æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»˜ããƒªãƒˆãƒ©ã‚¤**

**ææ¡ˆ: å®Ÿè£…æ™‚æœŸï¼ˆæœªå®šï¼šPhase Tä»¥é™ï¼‰**


```python
# ææ¡ˆã™ã‚‹ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹
async def parse_with_retry(
    response: str,
    expected_schema: dict,
    max_retries: int = 1,
) -> dict | None:
    """ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼æ™‚ã«ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã€‚"""

    for attempt in range(max_retries + 1):
        try:
            # æŠ½å‡ºã‚’è©¦è¡Œ
            json_match = re.search(r"[\[{].*[\]}]", response, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group())
                # ã‚¹ã‚­ãƒ¼ãƒã«å¯¾ã—ã¦ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                if validate_schema(parsed, expected_schema):
                    return parsed

        except json.JSONDecodeError as e:
            if attempt < max_retries:
                # ä¿®æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒªãƒˆãƒ©ã‚¤
                response = await llm_call(
                    f"Your previous response had a JSON error: {e}\n"
                    f"Original response: {response[:500]}\n"
                    f"Please output valid JSON matching this schema: {expected_schema}"
                )
            else:
                return None

    return None
```

### 4.2 ä¸è¶³: ã‚¹ã‚­ãƒ¼ãƒãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

**ç¾çŠ¶:** JSONã¯ãƒ‘ãƒ¼ã‚¹ã•ã‚Œã‚‹ãŒã‚¹ã‚­ãƒ¼ãƒã¯ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚Œãªã„ã€‚

**å•é¡Œ:** æ¬ è½ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã€å‹ä¸ä¸€è‡´ãŒæš—é»™çš„ã«å—ã‘å…¥ã‚Œã‚‰ã‚Œã‚‹ã€‚

**ææ¡ˆ: LLMå‡ºåŠ›ç”¨Pydanticãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ **

```python
# src/filter/llm_schemas.pyï¼ˆæ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
from pydantic import BaseModel, Field, validator

class ExtractedFact(BaseModel):
    fact: str = Field(..., min_length=10)
    confidence: float = Field(..., ge=0.0, le=1.0)
    evidence_type: str = Field(default="observation")

    @validator("evidence_type")
    def validate_evidence_type(cls, v):
        allowed = {"statistic", "citation", "observation"}
        return v if v in allowed else "observation"

class ExtractedClaim(BaseModel):
    claim: str = Field(..., min_length=10)
    type: str = Field(default="factual")
    relevance_to_query: float = Field(default=0.5, ge=0.0, le=1.0)
    confidence: float = Field(default=0.5, ge=0.0, le=1.0)
```

### 4.3 ä¸è¶³: å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¼·åˆ¶

**ç¾çŠ¶:** ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã€ŒOutput JSON onlyã€ã¨æŒ‡ç¤ºã—ã¦ã„ã‚‹ãŒå¼·åˆ¶ã•ã‚Œã¦ã„ãªã„ã€‚

**å•é¡Œ:** LLMãŒJSONå‰ã«å‰ç½®ããƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã™ã‚‹ã“ã¨ãŒå¤šã„ã€‚

**ææ¡ˆ: æ§‹é€ åŒ–å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰**

```python
# ã‚µãƒãƒ¼ãƒˆã™ã‚‹APIå‘ã‘ï¼ˆä¾‹: OpenAI, Anthropicï¼‰
response = await client.messages.create(
    model="claude-3-5-sonnet-20241022",
    messages=[...],
    # JSONå‡ºåŠ›ã‚’å¼·åˆ¶
    response_format={"type": "json_object"}
)
```

### 4.4 ~~ä¸è¶³: ä¿¡é ¼åº¦ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³~~ â†’ NLIå°‚ç”¨ã¨ã—ã¦æ—¢å­˜å®Ÿè£…ã‚ã‚Š

> **æ³¨æ„**: ä¿¡é ¼åº¦ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯ `src/utils/calibration.py` ã« **NLI ãƒ¢ãƒ‡ãƒ«å°‚ç”¨** ã¨ã—ã¦å®Ÿè£…æ¸ˆã¿ã€‚
> LLM æŠ½å‡º confidence ã¨ã®é–¢ä¿‚ã¯ [`docs/confidence-calibration-design.md`](./confidence-calibration-design.md) ã‚’å‚ç…§ã€‚

**ã‚¹ã‚³ãƒ¼ãƒ—:**
- **å¯¾è±¡**: `nli-confidence`ï¼ˆNLIãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ï¼‰
- **éå¯¾è±¡**: `llm-confidence`ï¼ˆLLMè‡ªå·±å ±å‘Šï¼‰â€” åˆ¥è¨­è¨ˆã§æ¤œè¨ä¸­

**æ—¢å­˜å®Ÿè£…:**
- Platt Scaling / Temperature Scaling
- Brier Score / ECEï¼ˆExpected Calibration Errorï¼‰è©•ä¾¡
- è‡ªå‹•åŠ£åŒ–æ¤œçŸ¥ + ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
- å¢—åˆ†å†ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚µãƒ³ãƒ—ãƒ«è“„ç©ãƒˆãƒªã‚¬ãƒ¼ï¼‰

**MCPãƒ„ãƒ¼ãƒ«:**
- `calibration_metrics(get_stats)`: ç¾åœ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨å±¥æ­´
- `calibration_metrics(get_evaluations)`: è©•ä¾¡å±¥æ­´
- `calibration_rollback`: ä»¥å‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¸ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

**å‚ç…§:**
- ADR-0011 (LoRA Fine-tuning Strategy)
- [`docs/confidence-calibration-design.md`](./confidence-calibration-design.md) â€” ç”¨èªå®šç¾©ã¨è¨­è¨ˆææ¡ˆ

### 4.5 æ¨å¥¨: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹é€ ã®æ¨™æº–åŒ–

**ææ¡ˆã™ã‚‹ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹é€ :**

```jinja2
{# SECTION 1: ãƒ­ãƒ¼ãƒ«ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ #}
You are a {{ role }} for {{ purpose }}.

{# SECTION 2: ã‚¿ã‚¹ã‚¯å®šç¾© #}
## Task
{{ task_description }}

{# SECTION 3: å…¥åŠ› #}
## Input
{{ input_variable }}

{# SECTION 4: åˆ¶ç´„ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ #}
{% if constraints %}
## Constraints
{% for c in constraints %}
- {{ c }}
{% endfor %}
{% endif %}

{# SECTION 5: å‡ºåŠ›ä»•æ§˜ #}
## Output Format
{{ output_schema }}

{# SECTION 6: ä¾‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ #}
{% if examples %}
## Example
{{ examples }}
{% endif %}

{# SECTION 7: æœ€çµ‚æŒ‡ç¤º #}
Output {{ output_format }} only:
```

---

## Part 5: å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Phase 0: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ•´åˆæ€§ï¼ˆå®Œäº†ï¼‰

> **å•é¡Œ:** `src/utils/prompt_manager.py` ã¨ `render_prompt()` ã«ã‚ˆã‚Šã€ŒLLMå…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ `config/prompts/*.j2` ã«å¤–éƒ¨åŒ–ã€ã¨ã„ã†æ§‹é€ ãŒæ—¢ã«ã‚ã‚‹ãŒã€ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ®‹ã£ã¦ã„ã‚‹ã€‚

| ã‚¿ã‚¹ã‚¯ | ç§»å‹•å…ƒ | ç§»å‹•å…ˆ |
|--------|--------|--------|
| Quality Assessmentå¤–éƒ¨åŒ– | `quality_analyzer.py` | `config/prompts/quality_assessment.j2` |
| Initial Summaryå¤–éƒ¨åŒ– | `chain_of_density.py` | `config/prompts/initial_summary.j2` |
| Densifyå¤–éƒ¨åŒ– | `chain_of_density.py` | `config/prompts/densify.j2` |

**æ–°è¦ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆå¾Œã®æ§‹æˆ:**
```
config/prompts/
â”œâ”€â”€ decompose.j2           # æ—¢å­˜
â”œâ”€â”€ detect_citation.j2     # æ—¢å­˜
â”œâ”€â”€ extract_claims.j2      # æ—¢å­˜
â”œâ”€â”€ extract_facts.j2       # æ—¢å­˜
â”œâ”€â”€ relevance_evaluation.j2 # æ—¢å­˜
â”œâ”€â”€ summarize.j2           # æ—¢å­˜
â”œâ”€â”€ translate.j2           # æ—¢å­˜
â”œâ”€â”€ quality_assessment.j2  # æ–°è¦ï¼ˆè³ªå• analyzer ã‹ã‚‰ç§»å‹•ï¼‰
â”œâ”€â”€ initial_summary.j2     # æ–°è¦ï¼ˆCoD ã‹ã‚‰ç§»å‹•ï¼‰
â””â”€â”€ densify.j2             # æ–°è¦ï¼ˆCoD ã‹ã‚‰ç§»å‹•ï¼‰
```

**Pythonã‚³ãƒ¼ãƒ‰å¤‰æ›´ä¾‹:**
```python
# Before (quality_analyzer.py)
LLM_QUALITY_ASSESSMENT_PROMPT = """You are an expert..."""
prompt = LLM_QUALITY_ASSESSMENT_PROMPT.format(text=text)

# After
from src.utils.prompt_manager import render_prompt
prompt = render_prompt("quality_assessment", text=text)
```

### Phase 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„ï¼ˆå…¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè‹±èªåŒ–ï¼‰

å…¨10ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è‹±èªåŒ–ã—ã€Part 2ã®æ”¹å–„æ¡ˆã‚’é©ç”¨ã™ã‚‹ã€‚

| ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ | Part 2å‚ç…§ | å¤‰æ›´å†…å®¹ |
|-------------|-----------|---------|
| `summarize.j2` | 2.3 | å…¨é¢æ›¸ãæ›ãˆï¼ˆè©•ä¾¡Dâ†’æ”¹å–„æ¡ˆï¼‰ |
| `extract_claims.j2` | 2.2 | å…¨é¢æ›¸ãæ›ãˆï¼ˆè©•ä¾¡Câ†’æ”¹å–„æ¡ˆï¼‰ |
| `extract_facts.j2` | 2.1 | å…¨é¢æ›¸ãæ›ãˆï¼ˆè©•ä¾¡Câ†’æ”¹å–„æ¡ˆï¼‰ |
| `translate.j2` | 2.4 | å…¨é¢æ›¸ãæ›ãˆï¼ˆè©•ä¾¡Dâ†’æ”¹å–„æ¡ˆï¼‰ |
| `densify.j2` | 2.8 | å…¨é¢æ›¸ãæ›ãˆï¼ˆè‹±èªåŒ–+æ”¹å–„æ¡ˆï¼‰ |
| `initial_summary.j2` | 2.10 | å…¨é¢æ›¸ãæ›ãˆï¼ˆæ”¹å–„æ¡ˆé©ç”¨ï¼‰ |
| `quality_assessment.j2` | 2.9 | å…¨é¢æ›¸ãæ›ãˆï¼ˆæ”¹å–„æ¡ˆé©ç”¨ï¼‰ |
| `decompose.j2` | 2.5 | è»½å¾®ä¿®æ­£ï¼ˆhintsã‚¬ã‚¤ãƒ€ãƒ³ã‚¹è¿½åŠ ã€è‹±èªåŒ–ï¼‰ |
| `detect_citation.j2` | 2.6 | è»½å¾®ä¿®æ­£ï¼ˆå­¦è¡“å¼•ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³è¿½åŠ ã€è‹±èªåŒ–ï¼‰ |
| `relevance_evaluation.j2` | 2.7 | è‹±èªåŒ–ã®ã¿ï¼ˆå‚ç…§ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼‰ |

### Phase 2: LLMå‡ºåŠ›ã®å‹å®‰å…¨åŒ–ï¼ˆå®Œäº†ï¼‰

**ã‚¹ã‚³ãƒ¼ãƒ—:** LLMå‡ºåŠ›ã®ã¿å¯¾è±¡ã€‚NLI/Embedding/Rerankerã¯æ—¢ã« `src/ml_server/schemas.py` ã§Pydanticå‹å®‰å…¨ã®ãŸã‚å¯¾è±¡å¤–ã€‚

| ã‚¿ã‚¹ã‚¯ | ãƒ•ã‚¡ã‚¤ãƒ« | å†…å®¹ |
|--------|---------|------|
| JSONæŠ½å‡ºå…±é€šåŒ– | `src/filter/llm_output.py`ï¼ˆæ–°è¦ï¼‰ | `extract_json()` + `parse_and_validate()`ï¼ˆã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼+ãƒªãƒˆãƒ©ã‚¤+DBè¨˜éŒ²ï¼‰ |
| ãƒ‘ãƒ¼ã‚µãƒ¼é©ç”¨ | `llm.py`, `claim_decomposition.py`, `chain_of_density.py`, `quality_analyzer.py` | æ—¢å­˜ã® `re.search()` ã‚’ç½®ãæ›ãˆ |
| Pydanticã‚¹ã‚­ãƒ¼ãƒ | `src/filter/llm_schemas.py`ï¼ˆæ–°è¦ï¼‰ | `ExtractedFact`, `ExtractedClaim` ç­‰ |
| ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¿®æ­£ãƒªãƒˆãƒ©ã‚¤ | `src/filter/llm_output.py` | æœ€å¤§1å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤ã€å¤±æ•—æ™‚ã¯ `llm_extraction_errors` ã«DBè¨˜éŒ²ï¼ˆå‡¦ç†ã¯ç¶šè¡Œï¼‰ |

### Phase 3: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆå®Œäº†ï¼‰

> **æ³¨:** è‹±èªåŒ–ã¯Phase 1ã§å®Œäº†ã€‚`output_lang` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å°å…¥ã—ãªã„ï¼ˆè‹±èªå›ºå®šï¼‰ã€‚

| ã‚¿ã‚¹ã‚¯ | ãƒ•ã‚¡ã‚¤ãƒ« | å†…å®¹ |
|--------|---------|------|
| ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ä½œæˆ | `tests/prompts/`ï¼ˆæ–°è¦ï¼‰ | ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æ–‡æ¤œè¨¼ã€ã‚µãƒ³ãƒ—ãƒ«å…¥å‡ºåŠ›ãƒ†ã‚¹ãƒˆ |

**å®Ÿè£…æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«:**
- `tests/prompts/conftest.py` - å…±æœ‰ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ï¼ˆsample_inputs, json_output_templatesï¼‰
- `tests/prompts/test_template_syntax.py` - æ§‹æ–‡æ¤œè¨¼ã€è‹±èªã®ã¿ãƒã‚§ãƒƒã‚¯ã€å®Œå…¨æ€§æ¤œè¨¼
- `tests/prompts/test_template_rendering.py` - ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã€JSONå½¢å¼æ¤œè¨¼ã€å¢ƒç•Œå€¤ãƒ†ã‚¹ãƒˆ

**å®Ÿè¡Œæ–¹æ³•:**
```bash
make test-prompts      # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆã®ã¿
make test-llm-output   # LLMå‡ºåŠ›ãƒ‘ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆï¼ˆPhase 2é–¢é€£ï¼‰
```

### ~~Phase 4: é«˜åº¦ãªæ©Ÿèƒ½~~ï¼ˆå‰Šé™¤ - å®Ÿè£…æ¸ˆã¿ã¾ãŸã¯åˆ¥è¨­è¨ˆï¼‰

> **æ³¨æ„**: ä»¥ä¸‹ã®æ©Ÿèƒ½ã¯ã™ã¹ã¦æ—¢å­˜å®Ÿè£…æ¸ˆã¿ã¾ãŸã¯åˆ¥è¨­è¨ˆæ–‡æ›¸ã§æ¤œè¨ä¸­ã®ãŸã‚ã€Phase 4ã¯ä¸è¦ã€‚

| å½“åˆã®ææ¡ˆ | çŠ¶æ…‹ | å‚ç…§ |
|------------|------|------|
| Confidence calibrationï¼ˆNLIï¼‰ | âœ… å®Ÿè£…æ¸ˆã¿ | `src/utils/calibration.py`, ADR-0011 |
| Confidence calibrationï¼ˆLLMï¼‰ | ğŸ“ åˆ¥è¨­è¨ˆ | [`confidence-calibration-design.md`](./confidence-calibration-design.md) |
| A/Bãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | âœ… å®Ÿè£…æ¸ˆã¿ | `src/search/ab_test.py`, ADR-0010 |
| ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚° | âœ… gitç®¡ç†ã§ååˆ† | `config/prompts/*.j2` |

**MCPãƒ„ãƒ¼ãƒ«ï¼ˆæ—¢å­˜ã€NLIå°‚ç”¨ï¼‰:**
- `calibration_metrics`: NLIçµ±è¨ˆå–å¾—ã€è©•ä¾¡å±¥æ­´
- `calibration_rollback`: NLIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

**ç”¨èªã®æ˜ç¢ºåŒ–:** [`confidence-calibration-design.md`](./confidence-calibration-design.md) ã‚’å‚ç…§

---

## Part 6: Phase 2 å®Ÿè£…æ–¹é‡

**è¿½åŠ æ—¥:** 2025-12-27
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:** ç¢ºå®š

### 6.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLMå‡ºåŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â”‚â”€â”€â”€â–¶â”‚  LLMå‘¼ã³å‡ºã— â”‚â”€â”€â”€â–¶â”‚ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£         â”‚  â”‚
â”‚  â”‚ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆâ”‚    â”‚  (Provider)  â”‚    â”‚ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (validate_llm_output)â”‚  â”‚
â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                  â”‚              â”‚
â”‚                                                  â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    JSONæŠ½å‡ºå±¤                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚  â”‚
â”‚  â”‚  â”‚ extract_jsonâ”‚ â† src/filter/llm_output.pyï¼ˆå…±é€šåŒ–ï¼‰     â”‚  â”‚
â”‚  â”‚  â”‚ (regex)     â”‚                                          â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                  â”‚              â”‚
â”‚                                                  â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯                     â”‚  â”‚
â”‚  â”‚              (ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤)                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 6.2 å®Ÿè£…æ–¹é‡

#### ã‚¹ã‚³ãƒ¼ãƒ—

**å¯¾è±¡:** LLMå‡ºåŠ›ã®ã¿

**å¯¾è±¡å¤–:** NLI / Embedding / Reranker
- ã“ã‚Œã‚‰ã¯æ—¢ã« `src/ml_server/schemas.py` ã§Pydanticã‚¹ã‚­ãƒ¼ãƒã«ã‚ˆã‚‹å‹å®‰å…¨ãŒå®Ÿè£…æ¸ˆã¿
- å‡ºåŠ›ã¯æ•°å€¤ãƒ»ãƒ©ãƒ™ãƒ«æ–‡å­—åˆ—ã§ã‚ã‚Šã€è‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆã§ã¯ãªã„ãŸã‚JSONæŠ½å‡ºä¸è¦

#### JSONæŠ½å‡ºå…±é€šåŒ–

`src/filter/llm_output.py` ã‚’æ–°è¨­ã—ã€JSONæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’å…±é€šåŒ–ã™ã‚‹ã€‚

**å®Ÿè£…:**

```python
# src/filter/llm_output.py
"""LLMå‡ºåŠ›ã‹ã‚‰JSONã‚’æŠ½å‡ºã™ã‚‹å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚"""
import json
import re
from typing import Any


def extract_json(text: str, expect_array: bool = False) -> dict | list | None:
    """LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºã€‚Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å¯¾å¿œã€‚

    Args:
        text: LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ
        expect_array: Trueã®å ´åˆJSONé…åˆ—ã‚’æœŸå¾…

    Returns:
        ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONã€ã¾ãŸã¯æŠ½å‡ºå¤±æ•—æ™‚ã¯None
    """
    if not text:
        return None

    # 1. ç›´æ¥ãƒ‘ãƒ¼ã‚¹
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError:
        pass

    # 2. ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ï¼ˆå„ªå…ˆï¼‰
    match = re.search(r"```(?:json)?\s*([\[\{].*?[\]\}])\s*```", text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass

    # 3. ç”ŸJSONï¼ˆè²ªæ¬²ãƒãƒƒãƒï¼‰
    pattern = r"\[.*\]" if expect_array else r"\{.*\}"
    match = re.search(pattern, text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group())
        except json.JSONDecodeError:
            pass

    return None
```

#### ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å‡¦ç†ãƒãƒªã‚·ãƒ¼

| ã‚±ãƒ¼ã‚¹ | å‡¦ç† |
|--------|------|
| è¤‡æ•°JSONãƒ–ãƒ­ãƒƒã‚¯ | æœ€é•·ãƒãƒƒãƒï¼ˆè²ªæ¬²ï¼‰ã‚’ä½¿ç”¨ |
| Markdownã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ | ` ```json...``` ` ã‚’å„ªå…ˆçš„ã«æŠ½å‡º |
| æŠ½å‡ºå¤±æ•—æ™‚ | `None` ã‚’è¿”ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯å‘¼ã³å‡ºã—å…ƒã®è²¬ä»»ï¼‰ |
| ãƒã‚¹ãƒˆã—ãŸJSON | å¤–å´ã®ãƒ–ãƒ©ã‚±ãƒƒãƒˆã‚’å–å¾— |
| ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆã•ã‚ŒãŸJSON | ãƒ‘ãƒ¼ã‚¹å¤±æ•— â†’ `None` |

#### ç½®ãæ›ãˆå¯¾è±¡

| ãƒ•ã‚¡ã‚¤ãƒ« | ç¾è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ | ç½®ãæ›ãˆå¾Œ |
|----------|-------------|-----------|
| `llm.py` | `re.search(r"\[.*\]"...)` | `extract_json(response, expect_array=True)` |
| `claim_decomposition.py` | `_parse_llm_response()` å†…ã®JSONæŠ½å‡º | åŒä¸Š |
| `chain_of_density.py` | `_parse_llm_response()` å†…ã®JSONæŠ½å‡º | `extract_json(response)` |
| `quality_analyzer.py` | ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³JSONãƒ‘ãƒ¼ã‚¹ | `extract_json(response)` |

#### ãƒªãƒˆãƒ©ã‚¤ï¼†ã‚¨ãƒ©ãƒ¼è¨˜éŒ²ãƒãƒªã‚·ãƒ¼

1. **ãƒªãƒˆãƒ©ã‚¤**: JSONæŠ½å‡ºå¤±æ•—ã¾ãŸã¯ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼å¤±æ•—æ™‚ã€æœ€å¤§1å›ã¾ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¿®æ­£ãƒªãƒˆãƒ©ã‚¤ã‚’å®Ÿè¡Œ
2. **1å›ãƒªãƒˆãƒ©ã‚¤ã—ã¦ã‚‚å¤±æ•—ã—ãŸå ´åˆ**:
   - DBã«ã€Œã‚¨ãƒ©ãƒ¼ã§å€¤ãŒå–ã‚Œãªã‹ã£ãŸã€ã“ã¨ã‚’è¨˜éŒ²ï¼ˆ`llm_extraction_errors`ï¼‰
   - å‡¦ç†ã¯æ­¢ã‚ãšã«ç¶šè¡Œï¼ˆæ¬¡ã®ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸/ã‚¿ã‚¹ã‚¯ã¸é€²ã‚€ï¼‰
   - ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«: `WARNING`
3. **ADR-0004ã¨ã®æ•´åˆæ€§**: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¿®æ­£ãƒªãƒˆãƒ©ã‚¤ã¯ã€ŒåŒã˜æ©Ÿæ¢°çš„æŠ½å‡ºã‚¿ã‚¹ã‚¯ã®å†è©¦è¡Œã€ã§ã‚ã‚Šã€ç¦æ­¢ã•ã‚Œã¦ã„ã‚‹ã€Œæˆ¦ç•¥çš„æ±ºå®šã€ã«ã¯è©²å½“ã—ãªã„

#### Pydanticã‚¹ã‚­ãƒ¼ãƒæ–¹é‡ï¼ˆå¯›å®¹ãƒ¢ãƒ¼ãƒ‰ï¼‰

- æ¬ è½ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è£œå®Œ
- å‹ä¸ä¸€è‡´ã¯å¤‰æ›ã‚’è©¦ã¿ã‚‹ï¼ˆ`str` â†’ `float` ç­‰ï¼‰
- å¤‰æ›ä¸å¯ã®å ´åˆã®ã¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼

```python
# src/filter/llm_schemas.py
from pydantic import BaseModel, Field

class ExtractedFact(BaseModel):
    fact: str = Field(..., min_length=5)
    confidence: float = Field(default=0.5, ge=0.0, le=1.0)
    evidence_type: str = Field(default="observation")

class ExtractedClaim(BaseModel):
    claim: str = Field(..., min_length=5)
    type: str = Field(default="fact")
    relevance_to_query: float = Field(default=0.5, ge=0.0, le=1.0)
    confidence: float = Field(default=0.5, ge=0.0, le=1.0)
```

#### DBã‚¹ã‚­ãƒ¼ãƒå¤‰æ›´

**ä¸è¦**ã€‚

- `relevance_to_query`: æŠ½å‡ºæ™‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ä¸€æ™‚å€¤ã§ã‚ã‚Šã€ãƒ¡ãƒ¢ãƒªå†…å‡¦ç†ã§ååˆ†
- `evidence_type`: extract_factsç”¨ã§ã‚ã‚Šclaimsãƒ†ãƒ¼ãƒ–ãƒ«ã¨ç„¡é–¢ä¿‚

LLMå‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨DBä¿å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯åˆ†é›¢å¯èƒ½ã€‚ä¸€æ™‚çš„ãªã‚¹ã‚³ã‚¢ã¯ãƒ‘ãƒ¼ã‚¹å¾Œã«ãƒ¡ãƒ¢ãƒªå†…ã§ä½¿ç”¨ã—ã€DBã«ã¯æ—¢å­˜ã‚«ãƒ©ãƒ ã®ã¿ä¿å­˜ã™ã‚‹ã€‚

---

## ä»˜éŒ²A: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–¢æ•°ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹

### `validate_llm_output()` â€” ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

**å ´æ‰€:** `src/filter/llm_security.py:validate_llm_output()`

```python
def validate_llm_output(
    text: str,
    expected_max_length: int | None = None,
    warn_on_suspicious: bool = True,
    system_prompt: str | None = None,
    mask_leakage: bool = True,
) -> OutputValidationResult:
```

**å®Ÿè¡Œã•ã‚Œã‚‹ãƒã‚§ãƒƒã‚¯:**
1. URLæ¤œå‡º (`http://`, `https://`, `ftp://`)
2. IPã‚¢ãƒ‰ãƒ¬ã‚¹æ¤œå‡º (IPv4, IPv6)
3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¼æ´©æ¤œå‡º (n-gramãƒãƒƒãƒãƒ³ã‚°)
4. å‡ºåŠ›åˆ‡ã‚Šè©°ã‚ï¼ˆæœŸå¾…æœ€å¤§ã®10å€ï¼‰
5. ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆãƒã‚¹ã‚­ãƒ³ã‚° (`[REDACTED]`)

### `sanitize_llm_input()` â€” å…¥åŠ›å‰å‡¦ç†

**å ´æ‰€:** `src/filter/llm_security.py:sanitize_llm_input()`

**7ã‚¹ãƒ†ãƒƒãƒ—ãƒ—ãƒ­ã‚»ã‚¹:**
1. Unicode NFKCæ­£è¦åŒ–
2. HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ‡ã‚³ãƒ¼ãƒ‰
3. ã‚¼ãƒ­å¹…æ–‡å­—é™¤å»
4. åˆ¶å¾¡æ–‡å­—é™¤å»
5. LYRAã‚¿ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å»
6. å±é™ºãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
7. é•·ã•åˆ¶é™

---

## ä»˜éŒ²B: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚ã«ã“ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨:

- [ ] **ãƒ­ãƒ¼ãƒ«å®šç¾©:** æ˜ç¢ºãªãƒšãƒ«ã‚½ãƒŠ/å°‚é–€æ€§ã‚’æŒ‡å®š
- [ ] **ã‚¿ã‚¹ã‚¯æ˜ç¤º:** ä¸€æ–‡ã§ã®ã‚¿ã‚¹ã‚¯èª¬æ˜
- [ ] **å…¥åŠ›ãƒ©ãƒ™ãƒ«:** å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’æ˜ç¢ºã«åŒºåˆ‡ã‚‹
- [ ] **å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒ:** æ­£ç¢ºãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŒ‡å®šï¼ˆJSONã€ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰
- [ ] **åˆ¶ç´„åˆ—æŒ™:** é•·ã•åˆ¶é™ã€ä»¶æ•°åˆ¶é™ã€é™¤å¤–æ¡ä»¶
- [ ] **ä¾‹ã®æä¾›:** è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«ã¯å°‘ãªãã¨ã‚‚1ã¤ã®few-shotä¾‹
- [ ] **è¨€èªçµ±ä¸€:** å…¨ä½“ã§å˜ä¸€è¨€èª
- [ ] **æœ€çµ‚æŒ‡ç¤º:** ã€ŒOutput X only:ã€ã§å‰ç½®ãã‚’æ¸›ã‚‰ã™
- [ ] **ä¿¡é ¼åº¦åŸºæº–:** ä¿¡é ¼åº¦ã‚’è¦æ±‚ã™ã‚‹å ´åˆã¯ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å®šç¾©
- [ ] **ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¯èƒ½:** å‡ºåŠ›ã‚’ãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ã«ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¯èƒ½
