# Lyra Environment Configuration
# Copy this file to .env and modify as needed.
# .env is gitignored and will not be committed.
#
# Environment variables override settings.yaml values.
# Format: LYRA_<SECTION>__<KEY>=value (double underscore for nesting)

# =============================================================================
# HYBRID MODE (WSL + Container)
# =============================================================================
# MCP server runs on WSL host, LLM/ML accessed via proxy container

# Proxy URL for hybrid mode (lyra container proxy server)
# LYRA_GENERAL__PROXY_URL=http://localhost:8080

# =============================================================================
# USER-SPECIFIC SETTINGS
# =============================================================================

# Chrome CDP connection (WSL2 -> Windows Chrome)
# Dynamic Worker Pool: Each worker gets its own Chrome instance
# Worker N connects to BASE_PORT + N (e.g., Worker 0 -> 9222, Worker 1 -> 9223)
# LYRA_BROWSER__CHROME_HOST=localhost
# LYRA_BROWSER__CHROME_BASE_PORT=9222
# LYRA_BROWSER__CHROME_PROFILE_PREFIX=Lyra-

# =============================================================================
# CONTAINER NETWORKING (Internal services)
# =============================================================================

# Ollama LLM server (container name - accessed via proxy)
# LYRA_LLM__OLLAMA_HOST=http://ollama:11434

# Tor SOCKS proxy (container name)
# LYRA_TOR__SOCKS_HOST=tor
# LYRA_TOR__SOCKS_PORT=9050
# LYRA_TOR__CONTROL_PORT=9051

# ML Server (internal network - lyra-internal)
# LYRA_ML__SERVER_URL=http://lyra-ml:8100

# =============================================================================
# ML SERVER SETTINGS (for lyra-ml container)
# =============================================================================

# Model names (defaults shown, uncomment to override)
# LYRA_ML__EMBEDDING_MODEL=BAAI/bge-m3
# LYRA_ML__RERANKER_MODEL=BAAI/bge-reranker-v2-m3
# LYRA_ML__NLI_MODEL=cross-encoder/nli-deberta-v3-small

# =============================================================================
# SCRIPT SETTINGS (for scripts/*.sh)
# =============================================================================

# Container name (used by all scripts)
# LYRA_SCRIPT__CONTAINER_NAME=lyra

# Container startup timeout in seconds
# LYRA_SCRIPT__CONTAINER_TIMEOUT=30

# Test completion detection threshold in seconds
# LYRA_SCRIPT__COMPLETION_THRESHOLD=5

# =============================================================================
# MAKE COMMANDS OUTPUT MODE
# =============================================================================

# JSON output for AI agents (affects all make commands)
# Set to "true" for machine-readable JSON output
# LYRA_OUTPUT_JSON=false

# =============================================================================
# TEST SAFETY SETTINGS
# =============================================================================

# Test layer for pytest (controls external service access)
# Values: unit, integration, e2e (default: not set = unit/integration)
# Only e2e tests can access external search services
# LYRA_TEST_LAYER=e2e

# Override: allow external search during pytest (DANGEROUS - may cause IP blocking)
# Only set this if you know what you're doing
# LYRA_ALLOW_EXTERNAL_SEARCH=1

# =============================================================================
# OPTIONAL SETTINGS
# =============================================================================

# Session tags (prompt injection defense)
# Default: disabled for 3B models (struggle with long prompts)
# Enable for larger models (7B+) that handle complex prompts better
# LYRA_LLM__SESSION_TAGS_ENABLED=true

# Log level: DEBUG, INFO, WARNING, ERROR
# LYRA_GENERAL__LOG_LEVEL=INFO

# =============================================================================
# CLAIMS EXTRACTION SETTINGS
# =============================================================================

# Claims extraction scope - controls which sources use LLM for claim extraction
# "authoritative": Only gov/edu/academic sources (minimal LLM usage, fastest)
# "reputable": + research orgs, major news, .org domains (balanced, recommended)
# "all": All sources including blogs/forums (max LLM usage, slowest)
# LYRA_SEARCH__CLAIMS_EXTRACTION_SCOPE=reputable

# =============================================================================
# PYTHON BUILD SETTINGS
# =============================================================================

# PyO3 forward compatibility for Python 3.14
# Required for building sudachipy (Japanese NLP tokenizer) on Python 3.14
# Background: PyO3 0.23.5 officially supports up to Python 3.13, but Python 3.14
# is compatible via the stable ABI (ABI3). This flag enables forward compatibility
# mode, allowing sudachipy to build successfully on Python 3.14.
# Without this setting, `uv sync` will fail when building sudachipy.
# PYO3_USE_ABI3_FORWARD_COMPATIBILITY=1
