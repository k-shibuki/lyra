# Lyra Environment Configuration
# Copy this file to .env and modify as needed.
# .env is gitignored and will not be committed.
#
# =============================================================================
# RULES (Single Source of Truth)
# =============================================================================
#
# Settings resolution order (lowest -> highest):
# 1) config/*.yaml (defaults)
# 2) config/local.yaml (your machine overrides)
# 3) environment variables
#
# Env key conventions (split on purpose):
# - Python Settings overrides:
#     LYRA_<ROOT>__<NESTED>__...  (double underscore for nesting)
#   Only these <ROOT> values are treated as config input:
#     general, task_limits, search, crawler, tor, browser, llm, ml, embedding,
#     ranking, nli, storage, notification, quality, circuit_breaker, metrics,
#     concurrency, academic_apis
#   Anything else is for scripts/tests only (not merged into Settings()).
#
# - Script/Make flags (do NOT affect Python Settings):
#     LYRA_OUTPUT_JSON, LYRA_QUIET, LYRA_COMPOSE_RUNTIME, LYRA_DISABLE_GPU, ...
#
# - Toolchain / build env (non-LYRA):
#     PYO3_USE_ABI3_FORWARD_COMPATIBILITY, HF_HUB_OFFLINE, ...

# NOTE:
# Put non-secret runtime settings in config/local.yaml, not in .env:
# - general.proxy_url, browser.chrome_*, llm.model/ollama_host, ml.server_url, tor.*, etc.

# =============================================================================
# CONTAINER RUNTIME
# =============================================================================

# Container runtime preference
# Values: podman (default), docker
# - Podman is preferred by default (better rootless security)
# - Set to "docker" if you prefer Docker or don't have Podman installed
LYRA_COMPOSE_RUNTIME=podman

# GPU mode (optional)
# Set to "1" to explicitly disable GPU passthrough and run in CPU-only mode.
# Useful when:
# - You want to test without GPU (even if nvidia-smi is available)
# - You want to avoid CDI/nvidia-container-toolkit setup
# - Running on a machine with incompatible/unsupported GPU
# Warning: CPU-only mode has significantly slower LLM/ML inference.
# LYRA_DISABLE_GPU=1

# =============================================================================
# SCRIPT SETTINGS (for scripts/*.sh)
# =============================================================================

# Container name (used by all scripts)
LYRA_SCRIPT__CONTAINER_NAME=proxy

# Container startup timeout in seconds
LYRA_SCRIPT__CONTAINER_TIMEOUT=30

# Test completion detection threshold in seconds
LYRA_SCRIPT__COMPLETION_THRESHOLD=5

# =============================================================================
# MAKE COMMANDS OUTPUT MODE
# =============================================================================

# JSON output for AI agents (affects all make commands)
# Set to "true" for machine-readable JSON output
# LYRA_OUTPUT_JSON=true

# Test JSON verbosity (only used when LYRA_OUTPUT_JSON=true)
# Values: full|minimal (default: full)
# LYRA_TEST_JSON_DETAIL=minimal

# Suppress non-essential output (affects all scripts)
# LYRA_QUIET=true

# Detail toggles (human mode)
# LYRA_DEV_STATUS_DETAIL=full
# LYRA_CHROME_STATUS_DETAIL=full
# LYRA_TEST_SHOW_TAIL_ON_SUCCESS=true

# =============================================================================
# TEST SAFETY SETTINGS
# =============================================================================

# Test layer for pytest (controls external service access)
# Values: unit, integration, e2e (default: not set = unit/integration)
# Only e2e tests can access external search services
# LYRA_TEST_LAYER=e2e

# Override: allow external search during pytest (DANGEROUS - may cause IP blocking)
# Only set this if you know what you're doing
# LYRA_ALLOW_EXTERNAL_SEARCH=1

# =============================================================================
# ACADEMIC API SETTINGS (Semantic Scholar / OpenAlex)
# =============================================================================
# These settings enable rate limit improvements for academic APIs.
# Without these settings, the system still works but with default rate limits.

# Semantic Scholar API Key (optional)
# Get your free API key at: https://www.semanticscholar.org/product/api
# Benefits: Dedicated rate limit (1 req/s per user)
# Without key: 1000 req/s shared across ALL anonymous users globally
#   (effectively much lower during busy periods - strongly recommend getting a key)
# LYRA_ACADEMIC_APIS__APIS__SEMANTIC_SCHOLAR__API_KEY=your_api_key_here

# OpenAlex / Semantic Scholar Contact Email (optional)
# Benefits:
#   - OpenAlex: Enters "polite pool" with better rate limits (10 req/s vs 1 req/s)
#   - Semantic Scholar: Included in User-Agent for identification
# Used in: mailto query parameter (OpenAlex) and User-Agent header (both)
# LYRA_ACADEMIC_APIS__APIS__OPENALEX__EMAIL=your_email@example.com
# LYRA_ACADEMIC_APIS__APIS__SEMANTIC_SCHOLAR__EMAIL=your_email@example.com

# =============================================================================
# PYTHON BUILD SETTINGS
# =============================================================================

# PyO3 forward compatibility for Python 3.14
# Required for building sudachipy (Japanese NLP tokenizer) on Python 3.14
# Background: PyO3 0.23.5 officially supports up to Python 3.13, but Python 3.14
# is compatible via the stable ABI (ABI3). This flag enables forward compatibility
# mode, allowing sudachipy to build successfully on Python 3.14.
# Without this setting, `uv sync` will fail when building sudachipy.
PYO3_USE_ABI3_FORWARD_COMPATIBILITY=1
