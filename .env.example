# Lyra Environment Configuration
# Copy this file to .env and modify as needed.
# .env is gitignored and will not be committed.
#
# Environment variables override settings.yaml values.
# Format: LYRA_<SECTION>__<KEY>=value (double underscore for nesting)

# =============================================================================
# HYBRID MODE (WSL + Container)
# =============================================================================
# MCP server runs on WSL host, LLM/ML accessed via proxy container

# Proxy URL for hybrid mode (lyra container proxy server)
LYRA_GENERAL__PROXY_URL=http://localhost:8080

# =============================================================================
# USER-SPECIFIC SETTINGS
# =============================================================================

# Chrome CDP connection (WSL2 -> Windows Chrome)
# Dynamic Worker Pool: Each worker gets its own Chrome instance
# Worker N connects to BASE_PORT + N (e.g., Worker 0 -> 9222, Worker 1 -> 9223)
LYRA_BROWSER__CHROME_HOST=localhost
LYRA_BROWSER__CHROME_BASE_PORT=9222
LYRA_BROWSER__CHROME_PROFILE_PREFIX=Lyra-

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# All model names should be configured here.
# To change models: edit these values, run `make setup-ml-models`, restart containers.

# LLM (Ollama) - auto-pulled on dev-up
# 3B: lighter, 7B: better instruction following for complex prompts
LYRA_LLM__MODEL=qwen2.5:3b

# Embedding model (HuggingFace) - auto-downloaded on dev-up
LYRA_ML__EMBEDDING_MODEL=BAAI/bge-m3

# NLI model (HuggingFace) - auto-downloaded on dev-up
LYRA_ML__NLI_MODEL=cross-encoder/nli-deberta-v3-small

# =============================================================================
# CONTAINER NETWORKING (Internal services)
# =============================================================================

# Ollama LLM server (container name - accessed via proxy)
LYRA_LLM__OLLAMA_HOST=http://ollama:11434

# ML Server (internal network - lyra-internal)
LYRA_ML__SERVER_URL=http://ml:8100

# Tor SOCKS proxy (container name)
LYRA_TOR__SOCKS_HOST=tor
LYRA_TOR__SOCKS_PORT=9050
LYRA_TOR__CONTROL_PORT=9051

# =============================================================================
# CONTAINER RUNTIME
# =============================================================================

# Container runtime preference
# Values: podman (default), docker
# - Podman is preferred by default (better rootless security)
# - Set to "docker" if you prefer Docker or don't have Podman installed
LYRA_COMPOSE_RUNTIME=podman

# GPU mode (optional)
# Set to "1" to explicitly disable GPU passthrough and run in CPU-only mode.
# Useful when:
# - You want to test without GPU (even if nvidia-smi is available)
# - You want to avoid CDI/nvidia-container-toolkit setup
# - Running on a machine with incompatible/unsupported GPU
# Warning: CPU-only mode has significantly slower LLM/ML inference.
# LYRA_DISABLE_GPU=1

# =============================================================================
# SCRIPT SETTINGS (for scripts/*.sh)
# =============================================================================

# Container name (used by all scripts)
LYRA_SCRIPT__CONTAINER_NAME=proxy

# Container startup timeout in seconds
LYRA_SCRIPT__CONTAINER_TIMEOUT=30

# Test completion detection threshold in seconds
LYRA_SCRIPT__COMPLETION_THRESHOLD=5

# =============================================================================
# MAKE COMMANDS OUTPUT MODE
# =============================================================================

# JSON output for AI agents (affects all make commands)
# Set to "true" for machine-readable JSON output
# LYRA_OUTPUT_JSON=true

# Test JSON verbosity (only used when LYRA_OUTPUT_JSON=true)
# Values: full|minimal (default: full)
# LYRA_TEST_JSON_DETAIL=minimal

# Suppress non-essential output (affects all scripts)
# LYRA_QUIET=true

# Detail toggles (human mode)
# LYRA_DEV_STATUS_DETAIL=full
# LYRA_CHROME_STATUS_DETAIL=full
# LYRA_TEST_SHOW_TAIL_ON_SUCCESS=true

# =============================================================================
# TEST SAFETY SETTINGS
# =============================================================================

# Test layer for pytest (controls external service access)
# Values: unit, integration, e2e (default: not set = unit/integration)
# Only e2e tests can access external search services
# LYRA_TEST_LAYER=e2e

# Override: allow external search during pytest (DANGEROUS - may cause IP blocking)
# Only set this if you know what you're doing
# LYRA_ALLOW_EXTERNAL_SEARCH=1

# =============================================================================
# ACADEMIC API SETTINGS (Semantic Scholar / OpenAlex)
# =============================================================================
# These settings enable rate limit improvements for academic APIs.
# Without these settings, the system still works but with default rate limits.

# Semantic Scholar API Key (optional)
# Get your free API key at: https://www.semanticscholar.org/product/api
# Benefits: Dedicated rate limit (1 req/s per user)
# Without key: 1000 req/s shared across ALL anonymous users globally
#   (effectively much lower during busy periods - strongly recommend getting a key)
# LYRA_ACADEMIC_APIS__APIS__SEMANTIC_SCHOLAR__API_KEY=your_api_key_here

# OpenAlex / Semantic Scholar Contact Email (optional)
# Benefits:
#   - OpenAlex: Enters "polite pool" with better rate limits (10 req/s vs 1 req/s)
#   - Semantic Scholar: Included in User-Agent for identification
# Used in: mailto query parameter (OpenAlex) and User-Agent header (both)
# LYRA_ACADEMIC_APIS__APIS__OPENALEX__EMAIL=your_email@example.com
# LYRA_ACADEMIC_APIS__APIS__SEMANTIC_SCHOLAR__EMAIL=your_email@example.com

# =============================================================================
# OPTIONAL SETTINGS
# =============================================================================

# Session tags (prompt injection defense)
# Default: disabled for 3B models (struggle with long prompts)
# Enable for larger models (7B+) that handle complex prompts better
# LYRA_LLM__SESSION_TAGS_ENABLED=true

# Log level: DEBUG, INFO, WARNING, ERROR
LYRA_GENERAL__LOG_LEVEL=INFO

# =============================================================================
# BROWSER UX / FOCUS POLICY
# =============================================================================
#
# Bring Chrome tab/window to front ONLY when user explicitly starts auth session (badge click).
# Default: false (avoid focus stealing)
# LYRA_BROWSER__BRING_TO_FRONT_ON_AUTH_SESSION_START=true

# Start Chrome minimized to reduce focus stealing on startup (WM-dependent).
# Default: false
# LYRA_BROWSER__CHROME_START_MINIMIZED=true

# =============================================================================
# CLAIMS EXTRACTION SETTINGS
# =============================================================================

# Claims extraction scope - controls which sources use LLM for claim extraction
# "authoritative": Only gov/edu/academic sources (minimal LLM usage, fastest)
# "reputable": + research orgs, major news, .org domains (balanced, recommended)
# "all": All sources including blogs/forums (max LLM usage, slowest)
LYRA_SEARCH__CLAIMS_EXTRACTION_SCOPE=reputable

# =============================================================================
# PYTHON BUILD SETTINGS
# =============================================================================

# PyO3 forward compatibility for Python 3.14
# Required for building sudachipy (Japanese NLP tokenizer) on Python 3.14
# Background: PyO3 0.23.5 officially supports up to Python 3.13, but Python 3.14
# is compatible via the stable ABI (ABI3). This flag enables forward compatibility
# mode, allowing sudachipy to build successfully on Python 3.14.
# Without this setting, `uv sync` will fail when building sudachipy.
PYO3_USE_ABI3_FORWARD_COMPATIBILITY=1
