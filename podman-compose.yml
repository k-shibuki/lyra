# Lancet Development Stack
# Usage: podman-compose up -d

version: "3.8"

services:
  # ============================================================
  # Lancet Development Container
  # ============================================================
  lancet:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: lancet
    env_file:
      - .env
    volumes:
      # Mount source code for development (rw for editing)
      - ./src:/app/src:rw
      - ./config:/app/config:ro
      - ./tests:/app/tests:rw
      # Persist data
      - ./data:/app/data:rw
      - ./logs:/app/logs:rw
      - ./models:/app/models:rw
    environment:
      - LANCET_ENV=development
      - LANCET_LOG_LEVEL=INFO
      - OLLAMA_HOST=http://ollama:11434
      - TOR_SOCKS_HOST=tor
      - TOR_SOCKS_PORT=9050
      - TOR_CONTROL_PORT=9051
      # Chrome connection is configured via .env file:
      # LANCET_BROWSER__CHROME_HOST=<WSL2_GATEWAY_IP>
    # Keep container running for exec access
    command: ["tail", "-f", "/dev/null"]
    stdin_open: true
    tty: true
    depends_on:
      - tor
      - ollama
    networks:
      - lancet-net
      - lancet-llm-internal  # Internal network for Ollama communication
    restart: unless-stopped

  # ============================================================
  # Tor - Anonymous Network
  # ============================================================
  tor:
    image: docker.io/dperson/torproxy:latest
    container_name: lancet-tor
    environment:
      - TOR_NewCircuitPeriod=30
      - TOR_MaxCircuitDirtiness=600
    ports:
      - "9050:9050"  # SOCKS proxy
      - "9051:9051"  # Control port
    networks:
      - lancet-net
    restart: unless-stopped

  # ============================================================
  # Ollama - Local LLM Runtime (GPU)
  # SECURITY: Isolated to internal network only (ยง4.4.1 L1)
  # - No host port exposure
  # - No internet access (internal network only)
  # - Accessible only from Lancet container
  # ============================================================
  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: lancet-ollama
    volumes:
      # Persist models
      - ./models/ollama:/root/.ollama:rw
    # SECURITY: No ports exposed to host (ยง4.4.1 L1)
    # ports:
    #   - "11434:11434"  # REMOVED - no host access allowed
    networks:
      - lancet-llm-internal  # Internal only - no internet access
    restart: unless-stopped
    # GPU support via CDI (requires nvidia-container-toolkit)
    # To enable: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
    devices:
      - nvidia.com/gpu=all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0

  # ============================================================
  # Redis - Cache (Optional)
  # ============================================================
  # redis:
  #   image: docker.io/redis:7-alpine
  #   container_name: lancet-redis
  #   ports:
  #     - "6379:6379"
  #   networks:
  #     - lancet-net
  #   restart: unless-stopped

networks:
  # External-capable network for Tor, browser, etc.
  lancet-net:
    driver: bridge

  # SECURITY: Internal-only network for Ollama (ยง4.4.1 L1)
  # - internal: true prevents any external/internet access
  # - Ollama can only communicate with containers on this network
  # - Prevents prompt injection attacks from exfiltrating data
  lancet-llm-internal:
    driver: bridge
    internal: true

# GPU support with Podman:
# 1. Install nvidia-container-toolkit
# 2. Generate CDI: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
# 3. Start: podman-compose up -d
# 4. Verify: podman exec lancet-ollama nvidia-smi

