# Lancet Development Stack
# Usage: podman-compose up -d

version: "3.8"

services:
  # ============================================================
  # Lancet Development Container
  # ============================================================
  lancet:
    build:
      context: .
      dockerfile: Dockerfile
      target: base  # Use base stage (no GPU packages) for development
    container_name: lancet
    env_file:
      - .env
    # Allow container to access WSL2 host via host.containers.internal
    # Required for Chrome CDP connection
    extra_hosts:
      - "host.containers.internal:host-gateway"
    volumes:
      # Mount source code for development (rw for editing)
      - ./src:/app/src:rw
      - ./config:/app/config:ro
      - ./tests:/app/tests:rw
      # Persist data
      - ./data:/app/data:rw
      - ./logs:/app/logs:rw
      - ./models:/app/models:rw
    environment:
      # Fixed values only - all other settings in .env
      - LANCET_ENV=development
      # ML Server URL (internal network)
    # Keep container running for exec access
    command: ["tail", "-f", "/dev/null"]
    stdin_open: true
    tty: true
    depends_on:
      - tor
      - ollama
      - lancet-ml
    networks:
      - lancet-net
      - lancet-internal  # Internal network for inference (Ollama/ML)
    restart: unless-stopped

  # ============================================================
  # Tor - Anonymous Network
  # ============================================================
  tor:
    image: docker.io/dperson/torproxy:latest
    container_name: lancet-tor
    environment:
      - TOR_NewCircuitPeriod=30
      - TOR_MaxCircuitDirtiness=600
    ports:
      - "9050:9050"  # SOCKS proxy
      - "9051:9051"  # Control port
    networks:
      - lancet-net
    restart: unless-stopped

  # ============================================================
  # Ollama - Local LLM Runtime (GPU)
  # SECURITY: Isolated to internal network only
  # - No host port exposure
  # - No internet access (internal network only)
  # - Accessible only from Lancet container
  # ============================================================
  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: lancet-ollama
    volumes:
      # Persist models
      - ./models/ollama:/root/.ollama:rw
    # SECURITY: No ports exposed to host
    # ports:
    #   - "11434:11434"  # REMOVED - no host access allowed
    networks:
      - lancet-internal  # Internal only - no internet access
    restart: unless-stopped
    # GPU support via CDI (requires nvidia-container-toolkit)
    # To enable: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
    devices:
      - nvidia.com/gpu=all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0

  # ============================================================
  # Lancet ML Server - Embedding/Reranking/NLI (GPU)
  # SECURITY: Isolated to internal network only
  # - No host port exposure
  # - No internet access (internal network only)
  # - Accessible only from Lancet container
  # ============================================================
  lancet-ml:
    build:
      context: .
      dockerfile: Dockerfile.ml
      target: gpu  # Use GPU variant
      args:
        # Model names from .env (downloaded at build time)
        - LANCET_ML__EMBEDDING_MODEL=${LANCET_ML__EMBEDDING_MODEL:-BAAI/bge-m3}
        - LANCET_ML__RERANKER_MODEL=${LANCET_ML__RERANKER_MODEL:-BAAI/bge-reranker-v2-m3}
        - LANCET_ML__NLI_FAST_MODEL=${LANCET_ML__NLI_FAST_MODEL:-cross-encoder/nli-deberta-v3-xsmall}
        - LANCET_ML__NLI_SLOW_MODEL=${LANCET_ML__NLI_SLOW_MODEL:-cross-encoder/nli-deberta-v3-small}
    container_name: lancet-ml
    env_file:
      - .env
    volumes:
      # Mount ML server code for development
      - ./src/ml_server:/app/src/ml_server:rw
    # SECURITY: No ports exposed to host
    # ports:
    #   - "8100:8100"  # REMOVED - no host access allowed
    networks:
      - lancet-internal  # Internal only - no internet access
    restart: unless-stopped
    # GPU support via CDI (shared with Ollama)
    devices:
      - nvidia.com/gpu=all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all

  # ============================================================
  # Redis - Cache (Optional)
  # ============================================================
  # redis:
  #   image: docker.io/redis:7-alpine
  #   container_name: lancet-redis
  #   ports:
  #     - "6379:6379"
  #   networks:
  #     - lancet-net
  #   restart: unless-stopped

networks:
  # External-capable network for Tor, browser, etc.
  lancet-net:
    driver: bridge

  # SECURITY: Internal-only network for inference (Ollama + ML)
  # - internal: true prevents any external/internet access
  # - Ollama and ML containers can only communicate with Lancet on this network
  # - Prevents prompt injection attacks from exfiltrating data
  # - GPU is shared between Ollama and ML via CDI (排他制御はスケジューラ)
  lancet-internal:
    driver: bridge
    internal: true

# GPU support with Podman:
# 1. Install nvidia-container-toolkit
# 2. Generate CDI: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
# 3. Start: podman-compose up -d
# 4. Verify: podman exec lancet-ollama nvidia-smi
