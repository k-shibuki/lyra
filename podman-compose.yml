# Lyra Development Stack
# Usage: podman-compose up -d

version: "3.8"

services:
  # ============================================================
  # Lyra Development Container
  # ============================================================
  # In hybrid mode, this container runs a proxy server that bridges
  # the WSL-based MCP server to the internal network (Ollama/ML).
  # ============================================================
  lyra:
    build:
      context: .
      dockerfile: docker/Dockerfile
      target: base  # Use base stage (no GPU packages) for development
    container_name: lyra
    env_file:
      - .env
    volumes:
      # Mount source code for development (rw for editing)
      - ./src:/app/src:rw
      - ./config:/app/config:ro
      - ./tests:/app/tests:rw
      # Configuration files (for development)
      - ./pyproject.toml:/app/pyproject.toml:ro
      - ./requirements.txt:/app/requirements.txt:ro
      # Persist data
      - ./data:/app/data:rw
      - ./logs:/app/logs:rw
      - ./models:/app/models:rw
    environment:
      # Fixed values only - all other settings in .env
      - LYRA_ENV=development
      # Proxy server targets (internal network hostnames)
      - OLLAMA_TARGET_URL=http://ollama:11434
      - ML_TARGET_URL=http://lyra-ml:8100
    # Proxy port for WSL MCP server access (localhost only for security)
    ports:
      - "127.0.0.1:8080:8080"
    # Run proxy server (enables hybrid WSL/container mode)
    # For development shell access, use: podman exec -it lyra bash
    command: ["python", "-m", "src.proxy.server"]
    stdin_open: true
    tty: true
    depends_on:
      - tor
      - ollama
      - lyra-ml
    networks:
      - lyra-net
      - lyra-internal  # Internal network for inference (Ollama/ML)
    restart: unless-stopped

  # ============================================================
  # Tor - Anonymous Network
  # ============================================================
  tor:
    image: docker.io/dperson/torproxy:latest
    container_name: lyra-tor
    environment:
      - TOR_NewCircuitPeriod=30
      - TOR_MaxCircuitDirtiness=600
    ports:
      - "9050:9050"  # SOCKS proxy
      - "9051:9051"  # Control port
    networks:
      - lyra-net
    restart: unless-stopped

  # ============================================================
  # Ollama - Local LLM Runtime (GPU)
  # SECURITY: Isolated to internal network only
  # - No host port exposure
  # - No internet access (internal network only)
  # - Accessible only from Lyra container
  # ============================================================
  ollama:
    image: docker.io/ollama/ollama:latest
    container_name: lyra-ollama
    volumes:
      # Persist models
      - ./models/ollama:/root/.ollama:rw
    # SECURITY: No ports exposed to host
    # ports:
    #   - "11434:11434"  # REMOVED - no host access allowed
    networks:
      - lyra-internal  # Internal only - no internet access
    restart: unless-stopped
    # GPU support via CDI (requires nvidia-container-toolkit)
    # To enable: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
    devices:
      - nvidia.com/gpu=all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0

  # ============================================================
  # Lyra ML Server - Embedding/Reranking/NLI (GPU)
  # SECURITY: Isolated to internal network only
  # - No host port exposure
  # - No internet access (internal network only)
  # - Accessible only from Lyra container
  # ============================================================
  lyra-ml:
    build:
      context: .
      dockerfile: docker/Dockerfile.ml
      target: gpu  # Use GPU variant
      args:
        # Model names from .env (downloaded at build time)
        - LYRA_ML__EMBEDDING_MODEL=${LYRA_ML__EMBEDDING_MODEL:-BAAI/bge-m3}
        - LYRA_ML__RERANKER_MODEL=${LYRA_ML__RERANKER_MODEL:-BAAI/bge-reranker-v2-m3}
        - LYRA_ML__NLI_FAST_MODEL=${LYRA_ML__NLI_FAST_MODEL:-cross-encoder/nli-deberta-v3-xsmall}
        - LYRA_ML__NLI_SLOW_MODEL=${LYRA_ML__NLI_SLOW_MODEL:-cross-encoder/nli-deberta-v3-small}
    container_name: lyra-ml
    env_file:
      - .env
    volumes:
      # Mount ML server code for development
      - ./src/ml_server:/app/src/ml_server:rw
    # SECURITY: No ports exposed to host
    # ports:
    #   - "8100:8100"  # REMOVED - no host access allowed
    networks:
      - lyra-internal  # Internal only - no internet access
    restart: unless-stopped
    # GPU support via CDI (shared with Ollama)
    devices:
      - nvidia.com/gpu=all
    environment:
      - NVIDIA_VISIBLE_DEVICES=all

  # ============================================================
  # Redis - Cache (Optional)
  # ============================================================
  # redis:
  #   image: docker.io/redis:7-alpine
  #   container_name: lyra-redis
  #   ports:
  #     - "6379:6379"
  #   networks:
  #     - lyra-net
  #   restart: unless-stopped

networks:
  # External-capable network for Tor, browser, etc.
  lyra-net:
    driver: bridge

  # SECURITY: Internal-only network for inference (Ollama + ML)
  # - internal: true prevents any external/internet access
  # - Ollama and ML containers can only communicate with Lyra on this network
  # - Prevents prompt injection attacks from exfiltrating data
  # - GPU is shared between Ollama and ML via CDI (排他制御はスケジューラ)
  lyra-internal:
    driver: bridge
    internal: true

# GPU support with Podman:
# 1. Install nvidia-container-toolkit
# 2. Generate CDI: sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
# 3. Start: podman-compose up -d
# 4. Verify: podman exec lyra-ollama nvidia-smi
