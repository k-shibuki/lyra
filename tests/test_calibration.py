"""
Tests for confidence calibration module.

Covers:
- PlattScaling: Logistic regression calibration
- TemperatureScaling: Single parameter scaling
- Brier score and ECE metrics
- Calibrator: Main calibration manager
- EscalationDecider: Model escalation decisions
"""

import pytest
import math
from datetime import datetime, timezone, timedelta
from pathlib import Path
from unittest.mock import MagicMock, patch
import tempfile

from src.utils.calibration import (
    CalibrationSample,
    CalibrationParams,
    CalibrationResult,
    PlattScaling,
    TemperatureScaling,
    brier_score,
    expected_calibration_error,
    Calibrator,
    EscalationDecider,
    calibrate_confidence,
    evaluate_calibration,
    fit_calibration,
    check_escalation,
    add_calibration_sample,
)


# =============================================================================
# Test Data
# =============================================================================

# Well-calibrated predictions (predictions â‰ˆ actual frequency)
CALIBRATED_PREDICTIONS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
CALIBRATED_LABELS = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]

# Overconfident predictions (predictions higher than actual)
OVERCONFIDENT_PREDICTIONS = [0.9, 0.8, 0.85, 0.75, 0.95, 0.7, 0.9, 0.8, 0.85, 0.9]
OVERCONFIDENT_LABELS = [0, 1, 0, 1, 1, 0, 0, 1, 1, 0]


# =============================================================================
# CalibrationSample Tests
# =============================================================================

class TestCalibrationSample:
    """Tests for CalibrationSample dataclass."""
    
    def test_create_sample(self):
        """Should create sample with all fields."""
        sample = CalibrationSample(
            predicted_prob=0.8,
            actual_label=1,
            logit=1.5,
            source="llm_extract",
        )
        
        assert sample.predicted_prob == 0.8
        assert sample.actual_label == 1
        assert sample.logit == 1.5
        assert sample.source == "llm_extract"
    
    def test_default_values(self):
        """Should have sensible defaults."""
        sample = CalibrationSample(
            predicted_prob=0.5,
            actual_label=0,
        )
        
        assert sample.logit is None
        assert sample.source == ""
        assert sample.timestamp is not None


# =============================================================================
# CalibrationParams Tests
# =============================================================================

class TestCalibrationParams:
    """Tests for CalibrationParams dataclass."""
    
    def test_to_dict(self):
        """Should convert to serializable dict."""
        params = CalibrationParams(
            method="temperature",
            temperature=1.5,
            source="test",
            samples_used=100,
        )
        
        d = params.to_dict()
        
        assert d["method"] == "temperature"
        assert d["temperature"] == 1.5
        assert d["source"] == "test"
        assert "fitted_at" in d
    
    def test_from_dict(self):
        """Should create from dict."""
        data = {
            "method": "platt",
            "platt_a": 1.2,
            "platt_b": -0.3,
            "source": "nli_judge",
            "samples_used": 50,
            "fitted_at": "2024-01-15T10:00:00+00:00",
        }
        
        params = CalibrationParams.from_dict(data)
        
        assert params.method == "platt"
        assert params.platt_a == 1.2
        assert params.platt_b == -0.3
        assert params.source == "nli_judge"


# =============================================================================
# PlattScaling Tests
# =============================================================================

class TestPlattScaling:
    """Tests for PlattScaling calibration."""
    
    def test_fit_improves_calibration(self):
        """Fitting should improve calibration on overconfident data."""
        # Convert probs to logits
        logits = [math.log(p / (1 - p + 1e-10)) for p in OVERCONFIDENT_PREDICTIONS]
        
        A, B = PlattScaling.fit(logits, OVERCONFIDENT_LABELS)
        
        # A and B should be non-trivial values
        assert A != 1.0 or B != 0.0
    
    def test_transform_valid_range(self):
        """Transform should output valid probabilities."""
        for logit in [-5, -1, 0, 1, 5]:
            prob = PlattScaling.transform(logit, 1.0, 0.0)
            assert 0.0 <= prob <= 1.0
    
    def test_transform_identity_when_no_scaling(self):
        """With A=1, B=0, should be identity sigmoid."""
        logit = 0.0
        prob = PlattScaling.transform(logit, 1.0, 0.0)
        
        assert abs(prob - 0.5) < 0.001


# =============================================================================
# TemperatureScaling Tests
# =============================================================================

class TestTemperatureScaling:
    """Tests for TemperatureScaling calibration."""
    
    def test_fit_returns_positive_temperature(self):
        """Temperature should be positive."""
        logits = [math.log(p / (1 - p + 1e-10)) for p in OVERCONFIDENT_PREDICTIONS]
        
        T = TemperatureScaling.fit(logits, OVERCONFIDENT_LABELS)
        
        assert T > 0
    
    def test_high_temperature_reduces_confidence(self):
        """Higher temperature should reduce extreme probabilities."""
        logit = 2.0  # High confidence
        
        prob_t1 = TemperatureScaling.transform(logit, 1.0)
        prob_t2 = TemperatureScaling.transform(logit, 2.0)
        
        # With higher temp, probability should be closer to 0.5
        assert abs(prob_t2 - 0.5) < abs(prob_t1 - 0.5)
    
    def test_transform_valid_range(self):
        """Transform should output valid probabilities."""
        for logit in [-5, -1, 0, 1, 5]:
            for T in [0.5, 1.0, 2.0]:
                prob = TemperatureScaling.transform(logit, T)
                assert 0.0 <= prob <= 1.0


# =============================================================================
# Metrics Tests
# =============================================================================

class TestBrierScore:
    """Tests for Brier score calculation."""
    
    def test_perfect_predictions(self):
        """Perfect predictions should have Brier score 0."""
        predictions = [0.0, 0.0, 1.0, 1.0]
        labels = [0, 0, 1, 1]
        
        score = brier_score(predictions, labels)
        
        assert score == 0.0
    
    def test_worst_predictions(self):
        """Completely wrong predictions should have high Brier score."""
        predictions = [1.0, 1.0, 0.0, 0.0]
        labels = [0, 0, 1, 1]
        
        score = brier_score(predictions, labels)
        
        assert score == 1.0
    
    def test_uncertain_predictions(self):
        """50% predictions should give Brier score 0.25."""
        predictions = [0.5, 0.5, 0.5, 0.5]
        labels = [0, 0, 1, 1]
        
        score = brier_score(predictions, labels)
        
        assert abs(score - 0.25) < 0.001
    
    def test_calibrated_lower_than_overconfident(self):
        """Well-calibrated predictions should have lower Brier score."""
        brier_calibrated = brier_score(CALIBRATED_PREDICTIONS, CALIBRATED_LABELS)
        brier_overconfident = brier_score(OVERCONFIDENT_PREDICTIONS, OVERCONFIDENT_LABELS)
        
        assert brier_calibrated < brier_overconfident


class TestExpectedCalibrationError:
    """Tests for ECE calculation."""
    
    def test_perfect_calibration(self):
        """Perfect calibration should have ECE 0."""
        # Predictions exactly match bin accuracy
        predictions = [0.5] * 10
        labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
        
        ece, bins = expected_calibration_error(predictions, labels)
        
        assert ece < 0.1  # Allow small numerical error
    
    def test_returns_bin_data(self):
        """Should return bin data for reliability diagram."""
        ece, bins = expected_calibration_error(
            OVERCONFIDENT_PREDICTIONS,
            OVERCONFIDENT_LABELS,
            n_bins=5,
        )
        
        assert len(bins) == 5
        assert all("accuracy" in b for b in bins)
        assert all("confidence" in b for b in bins)


# =============================================================================
# Calibrator Tests
# =============================================================================

class TestCalibrator:
    """Tests for Calibrator class."""
    
    @pytest.fixture
    def calibrator(self, tmp_path):
        """Create calibrator with temp storage."""
        with patch("src.utils.calibration.get_project_root") as mock_root:
            mock_root.return_value = tmp_path
            return Calibrator()
    
    def test_fit_temperature(self, calibrator):
        """Should fit temperature scaling."""
        samples = [
            CalibrationSample(predicted_prob=p, actual_label=l, source="test")
            for p, l in zip(OVERCONFIDENT_PREDICTIONS, OVERCONFIDENT_LABELS)
        ]
        
        params = calibrator.fit(samples, "test", method="temperature")
        
        assert params.method == "temperature"
        assert params.temperature != 1.0  # Should have adjusted
        assert params.samples_used == len(samples)
    
    def test_fit_platt(self, calibrator):
        """Should fit Platt scaling."""
        samples = [
            CalibrationSample(predicted_prob=p, actual_label=l, source="test")
            for p, l in zip(OVERCONFIDENT_PREDICTIONS, OVERCONFIDENT_LABELS)
        ]
        
        params = calibrator.fit(samples, "test", method="platt")
        
        assert params.method == "platt"
        assert params.samples_used == len(samples)
    
    def test_calibrate_returns_valid_prob(self, calibrator):
        """Calibrated probability should be valid."""
        samples = [
            CalibrationSample(predicted_prob=p, actual_label=l, source="test")
            for p, l in zip(OVERCONFIDENT_PREDICTIONS, OVERCONFIDENT_LABELS)
        ]
        calibrator.fit(samples, "test")
        
        for prob in [0.1, 0.5, 0.9]:
            calibrated = calibrator.calibrate(prob, "test")
            assert 0.0 <= calibrated <= 1.0
    
    def test_calibrate_without_params(self, calibrator):
        """Should return original prob if no calibration."""
        prob = calibrator.calibrate(0.8, "unknown_source")
        
        assert prob == 0.8
    
    def test_evaluate_returns_metrics(self, calibrator):
        """Should return evaluation metrics."""
        samples = [
            CalibrationSample(predicted_prob=p, actual_label=l, source="test")
            for p, l in zip(OVERCONFIDENT_PREDICTIONS, OVERCONFIDENT_LABELS)
        ]
        
        result = calibrator.evaluate(samples, "test")
        
        assert result.brier_score >= 0
        assert result.expected_calibration_error >= 0
        assert result.samples_evaluated == len(samples)
    
    def test_needs_recalibration_new_source(self, calibrator):
        """New sources without samples don't need calibration (can't calibrate yet)."""
        # No samples yet - can't calibrate
        assert calibrator.needs_recalibration("new_source") is False
        
        # Add enough samples - now needs calibration
        for _ in range(Calibrator.RECALIBRATION_THRESHOLD):
            calibrator._pending_samples.setdefault("new_source", []).append(
                CalibrationSample(predicted_prob=0.5, actual_label=1, source="new_source")
            )
        assert calibrator.needs_recalibration("new_source") is True
    
    def test_needs_recalibration_with_pending_samples(self, calibrator):
        """Should trigger recalibration when enough samples pending."""
        # Initially calibrated
        samples = [
            CalibrationSample(predicted_prob=p, actual_label=l, source="test")
            for p, l in zip(OVERCONFIDENT_PREDICTIONS, OVERCONFIDENT_LABELS)
        ]
        calibrator.fit(samples, "test")
        
        # No pending samples yet
        assert calibrator.needs_recalibration("test") is False
        
        # Add samples up to threshold
        for i in range(Calibrator.RECALIBRATION_THRESHOLD):
            calibrator._pending_samples.setdefault("test", []).append(
                CalibrationSample(predicted_prob=0.5, actual_label=1, source="test")
            )
        
        assert calibrator.needs_recalibration("test") is True
    
    def test_add_sample_accumulates(self, calibrator):
        """add_sample should accumulate pending samples."""
        calibrator.add_sample(0.8, 1, "test")
        calibrator.add_sample(0.6, 0, "test")
        
        assert calibrator.get_pending_sample_count("test") == 2
    
    def test_add_sample_triggers_recalibration(self, calibrator):
        """add_sample should trigger recalibration at threshold."""
        # Add samples up to threshold - 1
        for i in range(Calibrator.RECALIBRATION_THRESHOLD - 1):
            result = calibrator.add_sample(0.8, 1, "test")
            assert result is False  # Not yet
        
        # Add one more to trigger
        result = calibrator.add_sample(0.8, 1, "test")
        assert result is True  # Triggered
        
        # Pending should be cleared after recalibration
        assert calibrator.get_pending_sample_count("test") == 0


# =============================================================================
# EscalationDecider Tests
# =============================================================================

class TestEscalationDecider:
    """Tests for EscalationDecider class."""
    
    @pytest.fixture
    def decider(self, tmp_path):
        """Create decider with temp calibrator."""
        with patch("src.utils.calibration.get_project_root") as mock_root:
            mock_root.return_value = tmp_path
            calibrator = Calibrator()
            return EscalationDecider(calibrator)
    
    def test_escalate_low_confidence(self, decider):
        """Should escalate when confidence below threshold."""
        should_escalate, calibrated = decider.should_escalate(
            0.5, "test", threshold=0.7
        )
        
        assert should_escalate is True
    
    def test_no_escalate_high_confidence(self, decider):
        """Should not escalate when confidence above threshold."""
        should_escalate, calibrated = decider.should_escalate(
            0.9, "test", threshold=0.7
        )
        
        assert should_escalate is False
    
    def test_uses_calibrated_confidence(self, decider):
        """Should use calibrated confidence for decision.
        
        Fitting on overconfident data should produce calibration that
        adjusts high confidence values.
        """
        # Fit calibration that lowers confidence
        samples = [
            CalibrationSample(predicted_prob=p, actual_label=l, source="test")
            for p, l in zip(OVERCONFIDENT_PREDICTIONS, OVERCONFIDENT_LABELS)
        ]
        decider._calibrator.fit(samples, "test", method="temperature")
        
        # Original 0.9 should be calibrated
        should_escalate, calibrated = decider.should_escalate(
            0.9, "test", threshold=0.7
        )
        
        # Calibrated value should be returned (valid probability in [0, 1])
        assert 0.0 <= calibrated <= 1.0, f"Calibrated confidence should be valid probability, got {calibrated}"
        # The calibration params should have been applied
        params = decider._calibrator.get_params("test")
        assert params is not None, "Expected calibration params to be set"
        # If temperature != 1.0, calibration was applied
        if params.temperature != 1.0:
            # Just verify we got a valid result - actual value depends on fitting
            assert isinstance(should_escalate, bool), "should_escalate should be bool"


# =============================================================================
# MCP Tool Function Tests
# =============================================================================

class TestMCPToolFunctions:
    """Tests for MCP tool integration functions."""
    
    @pytest.mark.asyncio
    async def test_calibrate_confidence(self):
        """calibrate_confidence should return result."""
        with patch("src.utils.calibration.get_calibrator") as mock_get:
            mock_calibrator = MagicMock()
            mock_calibrator.calibrate.return_value = 0.75
            mock_calibrator.get_params.return_value = CalibrationParams(
                method="temperature",
                temperature=1.2,
            )
            mock_get.return_value = mock_calibrator
            
            result = await calibrate_confidence(0.8, "test")
            
            assert result["original"] == 0.8
            assert result["calibrated"] == 0.75
            assert result["has_calibration"] is True
    
    @pytest.mark.asyncio
    async def test_evaluate_calibration(self):
        """evaluate_calibration should return metrics."""
        with patch("src.utils.calibration.get_calibrator") as mock_get:
            mock_calibrator = MagicMock()
            mock_calibrator.evaluate.return_value = CalibrationResult(
                brier_score=0.15,
                expected_calibration_error=0.05,
                samples_evaluated=100,
            )
            mock_get.return_value = mock_calibrator
            
            result = await evaluate_calibration(
                "test",
                [0.5, 0.6],
                [1, 0],
            )
            
            assert result["brier_score"] == 0.15
    
    @pytest.mark.asyncio
    async def test_fit_calibration(self):
        """fit_calibration should return params."""
        with patch("src.utils.calibration.get_calibrator") as mock_get:
            mock_calibrator = MagicMock()
            mock_calibrator.fit.return_value = CalibrationParams(
                method="temperature",
                temperature=1.3,
                samples_used=50,
            )
            mock_get.return_value = mock_calibrator
            
            result = await fit_calibration(
                "test",
                [0.5, 0.6],
                [1, 0],
            )
            
            assert result["method"] == "temperature"
            assert result["temperature"] == 1.3
    
    @pytest.mark.asyncio
    async def test_check_escalation(self):
        """check_escalation should return decision."""
        with patch("src.utils.calibration.get_escalation_decider") as mock_get:
            mock_decider = MagicMock()
            mock_decider.should_escalate.return_value = (True, 0.55)
            mock_get.return_value = mock_decider
            
            result = await check_escalation(0.6, "test")
            
            assert result["should_escalate"] is True
            assert result["calibrated_confidence"] == 0.55
    
    @pytest.mark.asyncio
    async def test_add_calibration_sample(self):
        """add_calibration_sample should add sample and return status."""
        with patch("src.utils.calibration.get_calibrator") as mock_get:
            mock_calibrator = MagicMock()
            mock_calibrator.add_sample.return_value = False
            mock_calibrator.get_pending_sample_count.return_value = 5
            mock_get.return_value = mock_calibrator
            
            result = await add_calibration_sample(0.8, 1, "test")
            
            assert result["sample_added"] is True
            assert result["pending_samples"] == 5
            assert result["recalibrated"] is False


# =============================================================================
# Edge Cases
# =============================================================================

class TestEdgeCases:
    """Edge case tests."""
    
    def test_brier_score_empty(self):
        """Should handle empty lists."""
        # numpy handles this with warning/nan
        import numpy as np
        
        with np.errstate(all="ignore"):
            score = brier_score([], [])
        
        assert math.isnan(score)
    
    def test_fit_insufficient_samples(self, tmp_path):
        """Should handle insufficient samples gracefully."""
        with patch("src.utils.calibration.get_project_root") as mock_root:
            mock_root.return_value = tmp_path
            calibrator = Calibrator()
            
            samples = [
                CalibrationSample(predicted_prob=0.5, actual_label=1, source="test")
            ]
            
            params = calibrator.fit(samples, "test")
            
            # Should return default params
            assert params.temperature == 1.0
    
    def test_prob_to_logit_extreme_values(self, tmp_path):
        """Should handle extreme probabilities."""
        with patch("src.utils.calibration.get_project_root") as mock_root:
            mock_root.return_value = tmp_path
            calibrator = Calibrator()
            
            # Should not raise
            logit_low = calibrator._prob_to_logit(0.0)
            logit_high = calibrator._prob_to_logit(1.0)
            
            assert logit_low < 0
            assert logit_high > 0

