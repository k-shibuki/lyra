---
description: "Creating, reviewing, or modifying tests: test perspectives table, coverage, and mock conventions"
alwaysApply: false
---
# Test Conventions

## 1. Test Perspectives Table (Equivalence Partitioning / Boundary Values)

1. Before starting any test work, first present a Markdown "test perspectives table."
2. The table must include at least these columns: `Case ID`, `Input / Precondition`, `Perspective (Equivalence / Boundary)`, `Expected Result`, `Notes`.
3. Rows should cover normal, abnormal, and boundary value cases. For boundary values, include at minimum `0 / min / max / ±1 / empty / NULL`.
   Boundary value candidates (0 / min / max / ±1 / empty / NULL) that are meaningless per specification may be omitted with reason stated in `Notes`.
4. If perspective gaps are discovered later, update the table after self-review and add necessary cases.
5. For minor modifications to existing tests (message adjustments, minor expected value changes) that don't add new branches or constraints, creating/updating the test perspectives table is optional.

### Template Example

| Case ID | Input / Precondition | Perspective (Equivalence / Boundary) | Expected Result                               | Notes |
|--------|----------------------|---------------------------------------|----------------------------------------------|-------|
| TC-N-01 | Valid input A        | Equivalence – normal                 | Processing succeeds and returns expected value | -     |
| TC-A-01 | NULL                 | Boundary – NULL                      | Validation error (required field)            | -     |
| ...     | ...                  | ...                                   | ...                                          | ...   |

---

## 2. Test Code Implementation Policy

1. Implement **all** cases listed in the above table as automated tests.
2. Always include **at least as many failure cases** (validation errors, exceptions, external dependency failures) as success cases.
3. Cover the following perspectives in tests:
   - Normal cases (main scenarios)
   - Abnormal cases (validation errors, exception paths)
   - Boundary values (0, min, max, ±1, empty, NULL)
   - Invalid type/format inputs
   - External dependency failures (API / DB / messaging, etc., where applicable)
   - Exception types and error messages
4. **When adding new parameters/fields, don't complete with just "value verification."** Include at least one (preferably both) of the following so tests fail when added parameters are "not propagated/used anywhere":
   - **Propagation (wiring) verification**: Assert that downstream function/client/repository arguments include the added parameter (mock call argument verification, generated request/query verification, etc.)
   - **Behavior (effect) verification**: Assert that changing the added parameter value changes output/side effects/persistence/call content as specified (filter/sort/search conditions/flags, etc.)
5. Target 100% branch coverage, designing additional cases as needed.
   100% branch coverage is a "target value"; when not reasonably achievable, cover at least high-business-impact branches and main error paths.
   When branches remain uncovered, state the reason and impact in `Notes` or PR body.

---

## 3. Given / When / Then Comments

Every test case must have the following comment format:

```text
// Given: Preconditions
// When:  Action to execute
// Then:  Expected result/verification
```

Write comments directly above test code or within steps, keeping scenarios traceable for readers.

---

## 4. Exception/Error Verification

1. For cases where exceptions occur, explicitly verify exception **type** and **message**.
2. For validation-type abnormal cases, also verify error codes and field information if available.
3. When simulating external dependency failures, use stubs/mocks to verify expected exceptions/retries/fallbacks are called.

---

## 5. Execution Commands and Coverage

1. At the end of test implementation, always document **execution commands** and **coverage collection method** at the end of documentation or PR body.
   - Example: `npm run test`, `pnpm vitest run --coverage`, `pytest --cov=...`
2. Check branch/statement coverage, targeting 100% branch coverage (when not reasonably achievable, prioritize high-business-impact branches and main error paths).
3. Attach coverage report results (screenshots or summary) where possible.

---

## 6. Operational Notes

1. Reject test diffs that don't comply with this rule in review.
2. Even without external dependencies, **use mocks to simulate failures** for failure cases.
3. When new branches or constraints are added to test target specifications, update test perspectives table and test code simultaneously.
4. When automation is difficult, document reason and alternative means, reaching agreement with reviewer.
   Alternative means should include at minimum: target feature and risk, manual verification procedure, expected results, and log/screenshot storage method.
5. As a rule, PRs with meaningful changes to production code (spec additions, bug fixes, refactors affecting behavior) must include corresponding automated test additions or updates.
6. When test additions/updates are reasonably difficult, document the reason and alternative verification procedure (manual test procedure, etc.) in PR body and reach agreement with reviewer.
7. Even for refactors not intended to change behavior, verify that change locations are sufficiently covered by existing tests; add tests if insufficient.

---

## 7. Mock/Patch Conventions

1. **Apply patches at the use site**, not the definition site.
   - When a module imports a function with `from x import y`, patch `module_under_test.y`, not `x.y`.
   - Example: If `src/filter/evidence_graph.py` does `from src.storage.database import get_database`, patch `src.filter.evidence_graph.get_database`.
2. **When tests fail, fix the setup. Don't weaken the test.**
   - Removing/relaxing assertions to make tests pass conceals bugs and violates wiring/effect principles.
   - Investigate root cause (incorrect patch location, missing fixtures, cache residue, etc.) before fixing.
3. When testing code using global singletons or module-level caches, clear caches before/after tests or prepare dedicated fixtures.

---

## 8. DB Isolation Usage

| Situation | Mechanism to Use | Notes |
|-----------|-----------------|-------|
| pytest testing | `test_database` fixture (`tests/conftest.py`) | Auto setup/cleanup per test function |
| Script manual verification/debug | `isolated_database_path()` (`src/storage/isolation.py`) | Auto-deleted when `async with` block ends |

- In pytest, just writing fixture as argument isolates DB. No need to directly use `isolated_database_path`.
- In manual verification scripts, wrap with `async with isolated_database_path()` to avoid polluting `data/lyra.db`.

---

Follow this rule, always self-check for missing perspectives when designing and implementing tests. Also fix existing errors unrelated to changes.
